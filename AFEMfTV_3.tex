%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX version of DFG form 54.012 EN
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% DFG Bestaetigungsnummer: 20141001548940209703

\let\Div=\div
\renewcommand{\div}{\mathop{\mathrm{div}}}

\documentclass[enabledeprecatedfontcommands,cleardoublepage=empty,headsepline,twoside,11pt,DIV=15,BCOR=12mm,final]{scrartcl}
%twoside,
%\documentclass[twoside,11pt,]{scrartcl}\

\usepackage[colorlinks=true]{hyperref}
\usepackage[english]{babel}

\usepackage[right]{eurosym}
\usepackage{enumerate}

\usepackage{graphicx}
\usepackage{paralist}
\usepackage{rotating,multirow,booktabs}

\usepackage{amsmath,amssymb}
\usepackage{comment,xspace}
\usepackage[latin1]{inputenc}
\usepackage{todonotes}
%\usepackage{color}
\usepackage{subfigure}
\usepackage[footsepline,headsepline]{scrpage2}
\usepackage{typearea}

\usepackage{color,soul} % to highlight text
\usepackage{xcolor}

\KOMAoptions{headsepline=true}
\KOMAoptions{footsepline=true}
\KOMAoptions{cleardoublepage=empty}

\usepackage[resetlabels,labeled]{multibib}
\newcites{O}{Project-related publications}

\usepackage{calc}

%Define a reference depth. 
%You can choose either relative or absolute.
%--------------------------
\newlength{\DepthReference}
%\settodepth{\DepthReference}{g}%relative to a depth of a letter.
\setlength{\DepthReference}{0pt}%absolute value.

%Define a reference Height. 
%You can choose either relative or absolute.
%--------------------------
\newlength{\HeightReference}
%\settoheight{\HeightReference}{T}
%\setlength{\HeightReference}{6pt}


%--------------------------
\newlength{\Width}%

\newcommand{\MyColorBox}[4][black]%
{%
    %\settowidth{\Width}{#2}%
    \setlength{\Width}{#2}
    \setlength{\HeightReference}{#3}
    %\setlength{\fboxsep}{0pt}%
    {\hspace{#4}%
    \nolinebreak
    \colorbox{#1}%
    {%      
        \raisebox{-\DepthReference}%
        {%
                \parbox[b][\HeightReference+\DepthReference][c]{\Width}{~}%
        }%
    }%
    }
}



\newcommand{\note}[1]{\marginpar{%
    \fbox{\begin{minipage}{25mm}
       #1
    \end{minipage}}}}

\newcommand{\newf}[1]{}%{\color{red}{#1}}}

\newcommand\todoin[2][]{\todo[inline, caption={2do}, #1]{
 \begin{minipage}{\textwidth-4pt}#2\end{minipage}}}

\ohead{\textsf{\pagemark}}
\ofoot{\textsf{DFG Application ``Adaptive Methods for Free Discontinuity Problems''}}
\cfoot{}

\renewcommand{\titlepagestyle}{empty}

\usepackage[labeled]{multibib}
\newcites{A,B}%
{{Articles published by outlets with scientific quality assurance, book publications, and works accepted for publication but not yet published},{Other publications}}



\renewenvironment{description}
{\begin{list}{\epdes}{\topsep0mm \labelwidth0mm \leftmargin3mm %
  \itemsep1pt plus 4pt \topsep3pt \parsep1pt plus 4pt \labelsep3mm}}
{\end{list}}
\renewenvironment{itemize}
{\begin{list}{$\triangleright$}{\labelwidth-2mm \leftmargin3mm %
  \itemsep5pt plus 0pt  \topsep3pt \parsep1pt plus 4pt \labelsep2mm}}
{\end{list}}

% for having several reference lists...
\newcounter{savecount}
\newcommand{\ourbibitem}[1]{%
\stepcounter{savecount}\def\ourlabel{O\arabic{savecount}}\bibitem[\ourlabel]{#1}}


% a, A
\newcommand{\As}[1][s]{\ensuremath{\mathbb{A}_{#1}}}
\newcommand{\abs}[1]{\ensuremath{\left|#1\right|}}
\newenvironment{algotab}%
{~\ttfamily\begin{samepage}%
\begin{tabbing}%
 \hspace*{5mm}\=\hspace{3ex}\=\hspace{3ex}\=\hspace{3ex}\=\hspace{3ex}%
\=\hspace{3ex}\=\hspace{3ex}\=\hspace{3ex}\=\hspace{3ex}\kill}%
{\end{tabbing}\end{samepage}}
\newcommand{\astfem}{\textsf{ASTFEM}\xspace}
\newcommand{\alberta}{\textsf{ALBERTA}\xspace}
% b, B

\newcommand{\B}{\mathcal{B}}
\newcommand{\bilin}[2]{\B[#1,\,#2]}

% c, C
\newcommand{\C}{{\mathbb C}}
\newcommand{\cGq}[1][q]{\textsf{cG$(#1)$}\xspace}
\newcommand{\Cleq}{\ensuremath{\lesssim}}
\newcommand{\Cgeq}{\ensuremath{\gtrsim}}
% d, D
\newcommand{\definedas}{\mathrel{:=}}
\newcommand{\dGq}[1][q]{\textsf{dG$(#1)$}\xspace}
\newcommand{\DOF}{\textsf{DOF}\xspace}
\newcommand{\DOFs}{\textsf{DOFs}\xspace}
\newcommand{\dual}[3][]{\ensuremath{\langle #2,\, #3\rangle}_{#1}}
\newcommand{\dt}{{\rm d}t\xspace}
\newcommand{\dune}{\textsf{DUNE}\xspace}
\newcommand{\dunefem}{\textsf{DUNE-FEM}\xspace}

% e, E
\newcommand{\E}{{\mathbb E}}
\newcommand{\elm}{E}
\newcommand{\epde}{\textsf{ePDE}\xspace}
\newcommand{\epdes}{\textsf{ePDEs}\xspace}

% g, G
\newcommand{\grid}{\mathcal{G}}
\newcommand{\G}{\mathbb{G}}

% h, H
\newcommand{\helm}[1][\elm]{\ensuremath{h_{#1}}}

% i, I
\newcommand{\ie}{i.\,e.}

% n, N
\newcommand{\N}{{\mathbb N}}

% P, p
\newcommand{\ppde}{\textsf{pPDE}\xspace}
\newcommand{\ppdes}{\textsf{pPDEs}\xspace}

% Q, q
\newcommand{\Q}{{\mathbb Q}}

% r, R
\newcommand{\R}{{\mathbb R}}

% t, T
\newcommand{\Tn}[1][n]{\ensuremath{\mathcal{T}_{#1}}}
\newcommand{\taun}[1][n]{\ensuremath{\tau_{#1}}}
\newcommand{\tol}{\textsf{TOL}\xspace}

% u, U
\newcommand{\Un}[1][n]{\ensuremath{U_{#1}}}

% v, V
\newcommand{\V}{\ensuremath{\mathbb{V}}}
\newcommand{\VN}[1][N]{\ensuremath{\V_{\!#1}}}

% w, W
\newcommand{\W}{\ensuremath{\mathbb{W}}}
\newcommand{\WN}[1][N]{\ensuremath{\W_{\!#1}}}
\newcommand{\Wclass}{\ensuremath{\mathcal{W}}}

% z, Z
\newcommand{\Z}{{\mathbb Z}}



\newcommand{\bnew}[1]{\textcolor{blue}{#1}}



\newcommand{\sbt}{\,\begin{picture}(-1,1)(-1,-3)\circle*{2}\end{picture}\ }


\newcommand{\nnew}[1]{{\textcolor{red}{#1}}}



\begin{document}

\begin{titlepage}\sffamily
  \subject{DFG Research Grant Application} 
  \title{Adaptive Methods for Free Discontinuity Problems}% (for Image Reconstruction)}
  
  \author{Andreas Langer and Fernando Gaspoz}
  \date{Stuttgart, \today \\[50mm] Submission\\[50mm]
    Application period: 01.04.2020 --- 31.03.2023}
\end{titlepage}

\maketitle[-1]
\cleardoublepage
\setcounter{page}{1}
\pagestyle{scrheadings}


\section*{Applicants}

\subsection*{Principle Investigator}

\begin{flushleft}
  \textbf{Dr. Andreas Langer}
  \begin{tabbing}
    DFG reference number:\quad\=Phone:\quad\=\kill
    Position:\>  PostDoc\\
    %DFG reference number:\>SI 814/1-2\\[1mm]
    Contact:\> Numerische Mathematik f\"ur H\"ochstleistungsrechner\\ 
    \>Institut f\"ur Angewandte Analysis und Numerische Simulation\\
    \> Universit\"at Stuttgart\\
    \> Pfaffenwaldring 57 \\
    \> D-70569 Stuttgart\\[1mm]
    \> Email:\> andreas.langer@ians.uni-stuttgart.de\\[1mm]
 \end{tabbing}
\end{flushleft}


\begin{flushleft}
  \textbf{Dr. Fernando Gaspoz}
  \begin{tabbing}
    DFG reference number:\quad\=Phone:\quad\=\kill
    Position:\> PostDoc\\
    %DFG reference number:\>SI 814/1-2\\[1mm]
    Contact:\> Numerische Mathematik f\"ur H\"ochstleistungsrechner\\ 
    \>Institut f\"ur Angewandte Analysis und Numerische Simulation\\
    \> Universit\"at Stuttgart\\
    \> Pfaffenwaldring 57 \\
    \> D-70569 Stuttgart\\[1mm]
    \> Email:\> fernando.gaspoz@ians.uni-stuttgart.de\\[1mm]
 \end{tabbing}
\end{flushleft}

%\subsection*{Co-Investigator}
%
%\begin{flushleft}
%  \textbf{Prof. Dr. Kunibert G. Siebert}
%  \begin{tabbing}
%    DFG reference number:\quad\=Phone:\quad\=\kill
%    Position:\> Professor\\
%    %DFG reference number:\>SI 814/1-2\\[1mm]
%    Contact:\> Numerische Mathematik f\"ur H\"ochstleistungsrechner\\ 
%    \>Institut f\"ur Angewandte Analysis und Numerische Simulation\\
%    \> Universit\"at Stuttgart\\
%    \> Pfaffenwaldring 57 \\
%    \> D-70569 Stuttgart\\[1mm]
%    \> Email:\> kunibert.siebert@ians.uni-stuttgart.de\\[1mm]
% \end{tabbing}
%\end{flushleft}
\section*{Project Description}

\paragraph{Summary. \textcolor{green}{(0.5 pages)}} 

In many modern applications, like image reconstruction and image analysis, the solution of interest is described by a discontinuous function. Unfortunately, it is a priori not known where the discontinuities  are located and that has to be determined numerically. In fact, computing discontinuities wrongly or inaccurately could be either fatal (as in obstacle and tumour detection) or scientifically costly.  %One approach is to optimize a functional, which consists of a generalized, measure-valued, gradient term.
In order to preserve edges and discontinuities  in image processing, the minimization of a functional consisting of a the total variation term is common and widely used. Typical applications, where such an optimization problem is solved are for example: image denoising, image deblurring, and reconstruction of partial Fourier-data (magnetic resonance imaging). Note that all these examples are ill-posed inverse problems and hence the respective optimization problem is originated by regularization techniques, e.g., total variation regularization.

In this project we solve the constituted problems by constructing methods, which automatically adjust to the discontinuities of the underlying (observed) data. This adjustment leads to methods, which adaptively (in an iterative manner) reduce the reconstruction error, allowing to obtain the discontinuities more accurately and hence yield qualitatively better reconstructions, and \hl{keep or even reduce the computational complexity} \todo{is this true?}  of the underlying problem at the same time. Additionally to these attributes the derived methods will possess further desirable properties, as robustness with respect to the given data, they are guaranteed to converge to a more accurate solution than state-of-the-art methods, and they allow for different discretization techniques. 

The adaptive adjustment of the derived methods occur on two sides, on the regularization side, where the regularization is locally adjusted, and on the discretization side, where an optimal discretization is learned. Optimal in the sense of computational complexity and reconstruction quality. We derive criteria based on edge-indicators, a posteriori estimates and local residuals, which tell us, how the adjustment of the regularization and discretization should be performed. In particular, at discontinuities the regularization should be very small, preventing any smoothing, while the mesh should be very fine too, allowing to adjust the regularization very locally for depicting the discontinuities very precisely. Hence the mesh is adaptively refined at discontinuities and coarsened at parts where the solution is homogeneous.


% In particular at discontinuities the mesh should be very fine allowing for very little regularization only very locally to preserve discontinuities very precisely (or accurately?)\hl{In particular, at discontinuities the regularization should be very small, preventing any smoothing, while the mesh should be very fine, allowing for depicting the discontinuities very precisely.} \todo{do not like this phrasing}

In this vein, these adaptive methods are especially important in real-world applications, where the solution needs to be computed accurately, as they allow for a very precise solution while keeping the computational complexity on a manageable level. With the proposed research we will substantially foster the development and numerical analysis of optimal adaptive methods for non-smooth inverse imaging problems. %In addition, the investigation will contribute to a better understanding of the used models and their respective solvers.

%We will mainly focus on two aspects of adaptivity. Firstly we consider an adaptive discretization of the underlying problem. Note, that adaptive finite elements are well understood for (elliptic) partial differential equations and have gained a lot of attention. They have been used in numerous practical applications showing very robust performance. Convergence proofs and optimal decay rates in terms of degree of freedom are available. We will design and analyse adaptive finite element methods for non-smooth variational image models. In particular, we will develop an adaptive discretization environment for the reconstruction of only partly given images (image inapinting) and for determining motion in image sequences. These challenging topics require the investigation of appropriate finite element discretization and a posteriori estimates. 

%Secondly we will consider an adaptive regularization of the ill-posed inverse imaging problems.

The principal investigators and co-investigator have a long track record in research areas like mathematical image processing, inverse problems, parameter selection strategies, non-smooth optimisation, finite element methods, a posteriori estimates and adaptive methods. Furthermore, the mathematical problems involved in the project are quite suitable for postgraduate training.


%  analyze finite element methods for the minimization of the non-smooth and non-additive total variation with main focus on adaptive discretization.

%In order to preserve edges and discontinuities in images, in image reconstruction the minimization of a functional consisting of a the total variation term is common and widely used. Typical applications, where such a optimization problem is solved, are for example image denoising, image inpainting, and image deblurring. Due to better hardware in such applications one has to deal with high-dimensional images and data sets. Since most of the images have very uniform parts, a way to reduce the number of picture elements is to discretize by the use of different sizes of picture elements, i.e., using adaptive finite elements.

%Adaptive finite elements are well understood for (elliptic) partial differential equations and have gained a lot of attention. They have been used in numerous practical applications showing very robust performance. Convergence proofs and optimal decay rates in terms of degree of freedom are available.

%Real world applications in image reconstruction would tremendously profit from an adaptive discretization environment. However, the understanding of how to use adaptive finite elements as well as the analysis of respective methods in particular for non-smooth variational image reconstruction, is yet, in our opinion, in a very early stage. There exist only first approaches for clean images and image denoising of Gaussian noise.

%The proposed research project focuses on the design and analysis of adaptive finite element methods for non-smooth variational image reconstruction. We will develop an adaptive environment for the reconstruction of images containing a mixture of noise. Note, that in real world applications images are usually corrupted by more than one type of noise. This challenging topic requires the investigation of appropriate finite element discretization and a-posteriori estimates. 

%With the proposed research we will substantially foster the development and numerical analysis of optimal adaptive methods for non-smooth minimization problems for image reconstruction. In addition, the investigation will contribute to a better understanding of the used models and their respective solvers.

\section{State of the art and preliminary work}
\label{sec:state-prelim}

\subsection{State of the art \textcolor{green}{(6 pages)}}
\label{sec:state} 

\subsubsection{Free discontinuity problems}

In certain applications, the desired solution should contain discontinuities, whereas it is a priori not know at which locations they will occur. Typical examples of such applications are found in image reconstruction, image analysis, and material science. In image reconstruction, which is the application of interest in this project, one is interested in recovering an image from its observed corrupted data, whereas the corruption might be for example noise, blur or missing parts. %Image analysis addresses the task of analysing the given observation. Here one might be for example interesting in extracting or detecting certain features or objects in an image. In case of image sequences also the question of detecting the motion of certain objects is part of image analysis. In material science a typical example is the calculation of the propagation of a crack path. 
 In all these applications, in order to allow for a discontinuous solution, one usually minimizes a functional consisting of a total variation term. We recall, that for $u\in L^1(\Omega)$, where $\Omega \subset \R^2$ is a bounded open domain with Lipschitz boundary,
$$
V(u,\Omega):=\sup\left\{\int_{\Omega} u \operatorname{div} \vec\phi \ \text{d}x : \vec\phi \in [C_c^1(\Omega)]^2, \|{|\vec\phi|_{\ell^2}}\|_{L^\infty(\Omega)} \leq 1\right\}
$$
is the variation of $u$ in $\Omega$. Here, $L^q(\Omega)$, with $q\in [1,\infty]$, denotes the usual Lebesgue space \cite{AdaFou} and $C_c^l(\Omega)$, $l\in \N$, is the space of $l$-times continuously differentiable functions with compact support in $\Omega$. In the event that $V(u,\Omega)<\infty$ we denote $|Du|(\Omega)=V(u,\Omega)$ and call it the total variation of $u$ in $\Omega$; see \cite{AmbFusPal, Giu} for more details. If $u\in W^{1,1}(\Omega)$, then $|Du|(\Omega)= \int_{\Omega}|\nabla u| dx$. Further, $BV(\Omega)$ denotes the space of functions with bounded variation, i.e., $u\in BV(\Omega)$ if and only if $V(u,\Omega)<\infty$. The space $BV(\Omega)$ endowed with the norm $\|u\|_{BV} = \|u\|_{L^1(\Omega)} + |Du|(\Omega)$ is a Banach space \cite{Giu}. 

\paragraph{The total variation model}

In image reconstruction one is interested in recovering an unknown image from a corrupted observation. Hence, the obtained data $g$ might be described as
\begin{equation}\label{eq:g}
g=\mathcal{N}(T\hat{u}),
\end{equation}
where $\hat{u}$ is the unknown original image, $T$ is a linear bounded operator modelling the image formation device (e.g., blur), and $\mathcal{N}$ represents noise. 
Note that in reality in general not only one type of noise, but a mixture of several different types, such as Gaussian, and impulse noise, occur in an image. This contamination usually happens during image acquisition and image transmission. In this project we consider images which are corrupted by additive Gaussian noise, impulsive noise or a combination of these two type of noises.

The reconstruction of $\hat{u}$ from $g$ is usually an ill-posed inverse problem and thus regularization techniques are required \cite{EngHanNeu}. A good approximation may be obtained by minimizing a functional consisting of a data term, which enforces the consistency between the recovered and measured image, a regularization term to prevent over-fitting, and a parameter $\alpha>0$ weighting the importance of these two terms. Since we are aiming at preserving discontinuities and edges in an image, the total variation is typically chosen as a regularization technique. However, we note, that a more sophisticated regularizer, as the nonlocal total variation \cite{KinOshJon}, the total generalized variation \cite{BreKunPoc}, a higher order regularizer (see e.g. \cite{PapSch} and references therein), or a nonconvex regularizer might be used as well \cite{}. 

The choice of the data-term typically depends on the type of noise contamination. For Gaussian noise contaminated images a quadratic $L^2$-data-term is usually chosen and has been successfully used; see for example \cite{Cha, ChaDar, ChaLio, ChaGolMul, ComWaj, DarSig05, DarSig06, DauTesVes, DobVog, GolOsh, HinSta, Nes, OshBurGolXuYin, WeiBlaAub}. In this case, which we refer to as the $L^2$-model, an approximation of the unknown original image $\hat{u}$ is obtained for the observed data $g \in L^2(\Omega)$ by solving 
\begin{equation}\label{L2TV}
\min_{u\in BV(\Omega)} \|Tu-g\|_{L^2(\Omega)}^2 + \alpha |Du|(\Omega),
\end{equation}
where $T : L^2(\Omega) \to L^2(\Omega)$ is as in \eqref{eq:g}. 
%Let us discuss several image deformations with respect to the operator $T$, leading to different applications, relevant for this project.
%\begin{enumerate}[(a)]
%\item In case the image is only corrupted by noise one typically sets $T=I$ (image denoising).
%\item If the image is blurred then $T$ is a convolution operator (image deblurring).
%\item 
%\end{enumerate} 
%For image inpainting the observed image has missing parts $D\subset \Omega$, and hence $T$ is set to be the characteristic function of $\Omega\setminus D$, i.e., $Tu=\chi_{\Omega\setminus D}\cdot u$. \todo{Other relevant choices of $T$}%in case the observed image is blurry, $T$ represents the respective convolution (blurring) operator. 
 %Here and in the sequel we assume that $T$ is known.

The $L^2$-TV model usually does not yield a satisfactory restoration in the presence of impulse noise. This type of noise is usually constituted due to malfunctioning pixels in camera sensors, faulty memory locations in hardware, or transmission over noisy digital links. There are two commonly used models of impulse noise considered in the literature. The first one, called \emph{salt-and-pepper noise}, where the noisy image $g$ is given by
$$
g(x) = 
\begin{cases}
d_{\min} & \text{ with probability } s_1\in [0,1),\\
d_{\max} & \text{ with probability } s_2\in [0,1),\\
T \hat{u}(x)  & \text{ with probability } 1-s_1-s_2,
\end{cases}
$$
with $1-s_1-s_2>0$ \cite{ChaHoNik}. Here and in the rest we assume that $T \hat{u}$ is in the dynamic range $[d_{\min},d_{\max}]$, i.e., $d_{\min}\leq T\hat{u}\leq d_{\max}$. The second model is called \emph{random-valued impulse noise}, where $g$ is described as
$$
g(x) = 
\begin{cases}
c & \text{ with probability } s\in [0,1),\\
T \hat{u}(x)  & \text{ with probability } 1-s,
\end{cases}
$$
with $c$ being a uniformly distributed random variable in the image intensity range $[d_{\min},d_{\max}]$. For impulse noise contaminated images a more successful approach uses instead of a quadratic $L^2$-data-fidelity term a non-smooth $L^1$-data-fidelity term \cite{All, Nik2002, Nik2004}. That is, instead of \eqref{L2TV} one optimizes the following minimization problem
\begin{equation}\label{L1TV}
\min_{u\in BV(\Omega)} \|Tu-g\|_{L^1(\Omega)} + \alpha |Du|(\Omega),
\end{equation}
which we call the $L^1$-TV model. 

In case of images, which are contaminated by a mixture of Gaussian and impulse noise, in \cite{HintermullerLanger:13} an $L^1$-$L^2$-data-fidelity term has been introduced and shown to be suited to the task of removing this type of mixed noise. In this approach, which we call $L^1$-$L^2$-TV model, an image is restored by solving
\begin{equation}\label{L1L2TVmodel}
\min_{u\in BV(\Omega)} \lambda_1 \|T_1u-g_1\|_{L^1(\Omega)} +  \lambda_2 \|T_2 u-g_2\|_{L^2(\Omega)}^2 + \alpha |Du|(\Omega),
\end{equation}
where $T_i : L^2(\Omega) \to L^2(\Omega)$ is a bounded linear operator, $g_i\in L^2(\Omega)$ is a given datum, and $\lambda_i\geq 0$ for $i=1,2$ with $\lambda_1+\lambda_2>0$. For the case of removing a mixture of Gaussian and impulse noise from an image $g$ one typically sets $T_1=T_2$ and $g_1=g_2=g$ in \eqref{L1L2TVmodel}. In this setting it is easy to see that the $L^1$-$L^2$-$TV$ model \eqref{L1L2TVmodel} is a generalization of \eqref{L2TV} and \eqref{L1TV}. In particular, if we set $\lambda_2=0$ in \eqref{L1L2TVmodel} then we obtain the $L^1$-TV model while for $\lambda_1=0$ we obtain the $L^2$-TV model. Clearly the solution of \eqref{L1L2TVmodel} highly depends on the parameters $\lambda_1$ and $\lambda_2$. A suitable selection of the parameters is discussed in \cite{Lan2015} for the special case of the $L^1$-TV and $L^2$-TV model and in \cite{Lan2016} for the more general $L^1$-$L^2$-TV model.

\paragraph{Imaging tasks}

Let us shortly mention image reconstruction problems which are considered in this project.

\subparagraph{Image denoising}

Image denoising is the problem of removing the noisy part of an image. Typically random noise is modelled by a probability distribution, where the kind of distribution is chosen according to the application. A typical example is additive Gaussian noise. In this case an observed image $g$ in \ref{eq:g} can be written as $g = \hat{u} + \eta$, where $\eta$ represents the Gaussian noise. Note that $T=I$, if there is no additional image deformation.

Noises in images are caused by many different sources, for instance:
\begin{itemize}
\item in astronomical imaging: atmospheric inhomogeneity in terms of density, temperature, and so on;
\item in medical imaging: material inhomogeneity of tissues or organs;
\item in night vision: fluctuation in heat, temperature, and infrared radiation;
\item in general image acquisition: inherent thermal noises in electrooptical imaging devices, physical or chemical noises of the target systems to be imaged, and inhomogeneity of intermediate media.
\end{itemize}


Looking at such pictures makes it often very difficult in spotting the important features and patterns. Such failures could be either clinically fatal (as in tumour detection) or scientifically costly \cite{ChaShe}.


\subparagraph{Image deblurring}

The task of reconstructing a sharp image from its blurry observation is called im- age deblurring. This problem often arises in optical, medical, and astronomical applications, for example, in satellite imaging and remote sensing. There are three major categories of blurs \begin{inparaenum}[(i)]
\item the out-of-focus blur,
\item the motion blur, and 
\item the medium-induced blur, 
\end{inparaenum}
which are categorized according to their physical background, see \cite{ChaShe} for more details. In all different cases of blurry images $g$ is usually described as $g =T\hat{u}$, where $T$ is a blur operator modelled as a convolution $T\hat{u} = \kappa * u$ with kernel $\kappa$. If $g$ is additionally also corrupted by some noise, the observed image data is then generally modelled as in \eqref{eq:g}.

\subparagraph{Reconstruction from partial Fourier-data}

In magnetic resonance imaging (MRI), one aims to reconstruct an image which is only sampled by partial Fourier-data and additionally distorted by additive Gaussian noise. In this case the observed data $g$, defined over the Fourier domain, is written as in \eqref{eq:g}, where the data-formation operator is given by $T = S \circ F$. Here $F$ is a two-dimensional Fourier transform and $S$ represents a downsampling of the Fourier-data. 


\subsubsection{Adaptivity for total variation minimization}

\paragraph{Locally adaptive regularisation}

Note, that a scalar regularization parameter might not be the best choice for every image restoration problem, since images usually have large homogeneous regions as well as parts with a lot of details. Actually it seems obvious that $\alpha$ should be small in parts with small features in order to preserve the details. On the contrary $\alpha$ should be large in homogeneous parts to remove noise considerably. With such a choice of a spatially varying weight we expect better reconstructions than with a globally constant parameter, as demonstrated for example in \cite{DonHinRin,HinRin}. 
 This motivated to consider multi-scale total variation models with spatially varying parameters initially suggested in \cite{RudOsh}. The multi-scale version of the $L^2$-TV and $L^1$-TV model read as
\begin{equation}\label{L2TVmultimodel}
\min_{u\in BV(\Omega)}  \int_\Omega |Tu-g|^\tau dx + \int_\Omega \alpha(x) |Du|, \qquad \tau\in \{1,2\},
\end{equation}
where $\alpha: \Omega \to \R_+$ is a bounded continuous function. Similarly one may consider
\begin{equation}\label{L2TVmultimodel2}
\min_{u\in BV(\Omega)} \int_\Omega \lambda(x) |Tu-g|^\tau dx +  \int_\Omega |Du|, \qquad \tau\in \{1,2\},
\end{equation}
where $\lambda: \Omega \to \R_+$. In the sequel we refer to \eqref{L2TVmultimodel} and \eqref{L2TVmultimodel2} as the multi-scale $L^2$-TV model.

In \cite{StrCha1996} the influence of the scale of an image feature on the choice of $\alpha$ is studied and the obtained observations were later used in \cite{StrBloCha1997} to construct an updating scheme of $\alpha$. Based on \eqref{L2TVmultimodel2} in \cite{BerCasRouSol} a piecewise constant function $\lambda$, where the pieces are defined by a partitioning of the image due to a pre-segmentation, is determined. In particular, for each segment a scalar $\lambda_i$, $i=1,\ldots,\#$pieces, is computed by Uzawa's method \cite{Cia}.

Later it was noticed that stable choices of $\lambda$ (respectively $\alpha$) should incorporate statistical properties of the noise. In this vein, in \cite{AlmBalCasHar, DonHinRin, GilSocZee} for the problem \eqref{L2TVmultimodel2} automated update rules for $\lambda$ based on statistics of local constraints were proposed. In \cite{GilSocZee} a two level approach for variational denoising is considered, where in the first level noise and relevant texture are isolated in order to compute local constraints based on local variance estimation. In the second level a gradient descent method and an update formula for $\lambda(x)$ derived from the Euler-Lagrange equation is utilized. An adaptation of this approach to multiplicative noise can be found in \cite{LiNgShe}. For convolution type of problems in \cite{AlmBalCasHar} based on an estimate of the noise variance for each pixel an automatic updating scheme of $\lambda$ using Uzawa's method is created. This approach is improved in \cite{DonHinRin} by determining the fidelity weights due to the Gumbel statistic for the maximum of a finite number of random variables associated with localized image residuals and by incorporating hierarchical image decompositions, proposed in \cite{TadNezVes2004,TadNezVes2008}, to speed up the iterative parameter adjustment process. An adaptation of this approach to a total variation model with $L^1$ local constraints is studied in \cite{HinRin}. A different approach has been proposed in \cite{SutDelAuj} for image denoising only, where non-local means \cite{BuaColMor} are used to create a non-local data fidelity term. While in all these approaches the adjustment of $\lambda$ relies on the output of $T$ being a deteriorated image again, in \cite{HinLan2015} the method of \cite{DonHinRin} is adjusted to the situation where $T$ is an orthogonal wavelet transform or Fourier transform. Recently also bilevel optimisation approaches are considered for computing spatially adaptive weights \cite{ChuDeLSch,HinRau,HinRauWuLan}.

%While all these approaches are related to image denoising, nearly no attention has been given to the applications in image inpainting and motion estimation. To the best of our knowledge, the only attempts for these tasks in this direction are \cite{BelHec2016,TheBelKalMoa,BelKalMoaThe}, where smoothed approximations of $L^2$-TV model have been considered together with an adaptive finite element discretization. In particular, the adaptive discretization as well as the distributed parameter selection are based on the considerations in \cite{BouCha}. However, these proposed adjustment strategies do not seem to be well understood yet, as till now no usable stopping criterion of the iterative process is provided. We believe that a locally adaptive treatment of the parameters in image inpainting and optical flow computation is able to improve the quality of the solution dramatically, as demonstrated already for image denoising. Therefore in this project one main topic is the development, analysis and implementation of locally adaptive parameter selection strategies, see Section \ref{Sec:Objective} for more details. \todo{Add for which application}%, for image segmentation it is not clear to us at all and for the calculation of the crack propagation this seems even uninteresting since there the parameters constitute from physical properties of the material.


\paragraph{Adaptive discretization}
For the inverse problem of reconstructing an image from an observed corrupted data, for example, by solving a problem of the type \eqref{L1L2TVmodel}, usually the finite difference method is used. The reason for this might be that the image information is usually stored as a set of pixels, where each pixel is a square, in a row-column format and that in such optimization problems one usually minimizes a non-smooth functional, which do not allow usual derivatives and makes it rather difficult to find its weak formulation, which is needed for a finite element method (FEM). Nevertheless, first attempts for using FEMs in image processing (restoration) have been made for non-linear diffusion models, which are used mainly for image denoising and edge detection. In particular, in \cite{BazBlo, BaeMik, PreRum} adaptive FEMs are proposed for the modified Perona-Malik model introduced in \cite{CatLioMorCol}, and in \cite{Li} an optimal error estimate for a finite element discretization of this diffusion model is derived.

Based on these considerations FEMs for smoothed versions of \eqref{L2TV}, where the total variation term is either replaced by a Tikhonov regularizer \cite{Lam} or a smoothed total variation \cite{DesJaoSel, Yao}, are introduced and investigated for image denoising. However, recently, based on a primal-dual formulation and the method proposed by Chambolle and Pock \cite{ChaPoc}, FE discretizations of \eqref{L2TV} have been directly tackled in \cite{Bar2012, Bar2013, TiaYua}. Motivated by these approaches, in \cite{BarNocSal} a FE discretization of the total variation flow is considered as well. 
Very recently a FEM for the minimization of a functional consisting of a combined $L^1/L^2$ data-fidelity term and a total variation regularisation utilizing the primal-dual method of \cite{ChaPoc} is presented in \cite{AlkLan}, where the implementation is performed in DUNE-ACFem.
%While to the best of knowledge no attention with respect to a FE discretization has been given to the $L^1$-TV model, 
%\todo{Ueber\-arbeiten!!!}


In order to diminish the dimensionality of images without losing important information of the analog image, a good way would be to discretize (sample) in an adaptive way by taking features of the pictured environment into account. Note, that images usually have homogeneous regions as well as parts with a lot of details. Hence, it seems intuitively sufficient to discretize fine in regions of transitions and fine details, while a coarse discretization should do well in uniform parts. In this vein, based on the primal-dual method of \cite{ChaPoc} in \cite{Bar2015} an adaptive FEM using a posteriori error estimates for \eqref{L2TV} is proposed.  %one may use an adaptive finite element method relying on a posteriori error estimates indicating whether an element has to be further separated or not. 
%Adaptive finite element methods have been widely used in solving partial differential equations due to its obvious success in reducing the complexity of problems \cite{AinOde,BanRan, Car}. Recently adaptive meshing strategies are also used for solving constrained optimization problems \cite{}. However, for the inverse problem of reconstructing an image from an observed corrupted data finite element methods not to mention adaptive finite element methods are not yet widely used. The reason for this might be that in such problems one usually minimizes a non-smooth functional, which do not allow usual derivatives and renders the optimality conditions complicated. Nevertheless, first attempts for using FEM have been made for smoothed versions of \eqref{L2TV}, where the total variation term is either replaced by a Tikhonov regularizer \cite{Lam} or a smoothed total variation \cite{DesJaoSel, Yao}. In the last years, based on the primal-dual method by Chambolle and Pock \cite{ChaPoc}, FE discretizations of \eqref{L2TV} have been directly tackled in \cite{Bar12, Bar13, TiaYua}. Based on this approach in \cite{Bar15} an aFEM using a posteriori error estimates for \eqref{L2TV} is proposed. 
Note that in \cite{Bar2015} only clean images are considered and no image reconstruction problem is tackled. In a similar way relying on a semi-smooth Newton method for minimizing \eqref{L2TV} over the Hilbert space $H^1_0(\Omega)$ in \cite{HinRin2014} an adaptive FEM, again using a posteriori error estimates, is suggested for image denoising. Hence these two approaches seem to be the only ones where adaptive FEMs for the minimization of the non-smooth total variation is considered. 
However, to the best of our knowledge, there exist no approach which combines adaptive meshing with local adaptive parameter selection, neither in a finite difference setting nor in a finite element setting. We are not even aware of any adaptive finite difference method for minimizing total variation.
%, adaptive meshing strategies for smooth imaging problems have been considered for non-linear diffusion models \cite{BazBlo, BaeMik, PreRum} (as already above mentioned) and optical tomography, see \cite{BanJos} and references therein.

\paragraph{A posteriori estimates}
Any adaptive strategy is based upon information encoded \newf{in local  estimators of the error notion at hand}. There exists a variety of reliable and efficient estimators for a large class of different elliptic problems; compare with the monographs~\cite{AinOde,Ciarlet:02,Verfurth:96} and the references therein. In this case error estimation is in general a two-step procedure: in a first step one shows, by a continuous inf-sup condition and continuity of the associated bilinear form, equivalence of the true error to some negative norm of the residual, then the non-computable negative norm of the residual is estimated by an equivalent (up to oscillation) but computable quantity in the second step. It is important to notice that the equivalence of the first step is equivalent to have the continuous inf-sup and at the same time continuity of B with respect to the chosen spaces. Recently a posteriori estimators have been developed  for solving constrained optimization problems \cite{KoRoSi:14}. However, for the inverse problem of reconstructing an image from an observed corrupted data a posteriori estimators for finite element methods are not yet widely used. The reason for this might be that in such problems one usually minimizes a non-smooth functional, which do not allow usual derivatives and renders the optimality conditions complicated. To our knowledge the only result in this direction is~\cite{Bar2015} for the $L^2$-TV model 
%(should I mention Baensch-Mikula? they do coarsening) 
in which an abstract version of Repin's technique~\cite{Repin} is applied.

%A posteriori estimation for Finite Element Methods has a long history and a vast literature~\textcolor{red}{(cite Verf\"urth, Ainsworth-Oden, Ciarlet, etc)}, although not much is found when we restrict ourselves to non-smooth problems of the type~\eqref{L2TV}.\todo{continue from here!!!}


\paragraph{Grid adaptation strategies}
Based on a posteriori error indicators, adaptive schemes modify the discretization with the objective to construct more accurate approximations. Mesh modifications (refinement and coarsening) strongly influence the quality of the discrete solution and therefore it is of utter importance to understand the effects of each type of adaptation on the discrete solution.

The literature show us that in the case of linear elliptic equations it is enough to rely only on refinement. Roughly speaking the restriction to refinement leads to nested spaces (as in the case of \emph{Newest Vertex Bisection}) or "almost" nested spaces (as in the case of \emph{Red-Green Refinement}) (see~\cite{Verfurth:96}), and in combination with a best approximation property of the solution implies an asymptotally decreasing sequence of errors. This is also reflected in practical experience which shows that refinement improves accuracy of the discrete solution and does not spoil it.

In the case  of non-smooth problems the works~\cite{GuMoNo,Bar2015} show an improvement of accuracy with respect to degrees of freedom in the case of Mumford-Shah and $L^2$-TV cases. %The general case of $L^1$-$L^2$-TV type has not been studied yet in adaptive meshes.

It is worth mentioning too  the work~\cite{BaeMik} in which adaptive coarsening is applied in order to improve efficiency of image smoothing by nonlinear diffusion of a Perona-Malik model.




\subsection{Preliminary work \textcolor{green}{(2 pages)}}\label{sec:prelim}

We next describe our activities related to the proposed research. Publications closely related to the project are indicated by a leading O.
\subsubsection{Locally adaptive parameter selection}

%Recently new methods for the automatic selection of parameters in the $L^2$-TV, $L^1$-TV and $L^1$-$L^2$-TV model have been proposed \citeO{Langer:17, HiLaRaWu:18, Langer:17a, Langer:18, Langer:19, HiRaWuLa:17}.


While the methods in \cite{DonHinRin, HinRin}, based on \eqref{L2TVmultimodel2} are competitive, the iterative adjustment of $\lambda$ requires, that $T$ maps into the image domain $\Omega$. This limits the applicability of these approaches. In particular, applications in which the operator $T$ is a transformation of the image data into some data space disparate of the image space, e.g., $T$ being a wavelet or Fourier transform, these methods cannot be used. In \citeO{HiLaRaWu:18} we have constructed a new locally adaptive parameter selection strategy which overcome this shortcomings. In particular, we have replaced \eqref{L2TVmultimodel2} for $\tau=2$ by a sequence of auxiliary problems. These auxiliary problems are chosen in a way, that
\begin{inparaenum}[(i)]
\item $T$ does not affect $u$ any longer and hence each auxiliary problem is solved in the spatial domain, and
\item the sequence of solutions of the auxiliary problems converges towards the minimizer of the original problem.
\end{inparaenum}
Since the sequence of auxiliary problems is solved in the spatial domain, we are able to choose the regularization parameter locally varying, in a similar way as in \cite{DonHinRin}.  Numerical experiments demonstrate, that this new method is able to generate reconstructions from partial Fourier data, as they occur in magnetic resonance imaging, and from partial wavelet coefficients, which are qualitatively better than the results of standard methods.

Instead of selecting the locally dependent weight $\lambda$ in \eqref{L2TVmultimodel2} in \citeO{Langer:17} a fully automatic selection strategy of locally adaptive parameters $\alpha$ for the problem \eqref{L2TVmultimodel} is presented. There the adjustment of $\alpha$ is based on local constraints, similarly as already considered for example in \cite{AlmBalCasHar,DonHinRin,HinRin}. However, while the iterative update of the spatially varying parameter in \cite{AlmBalCasHar,DonHinRin,HinRin} depends on an additional parameter, which needs to be chosen by the user, the approach in \citeO{Langer:17} does not need any additional parameter and hence is fully automatic for the user. Moreover, differently to the approaches in \cite{DonHinRin,HinRin} where the initial regularization parameter $\lambda>0$ has to be set sufficiently small, in our approach any initial $\alpha>0$ is allowed. In this sense is our algorithm in \citeO{Langer:17} even more general than the ones presented in \cite{DonHinRin,HinRin}.

The method developed in \citeO{Langer:17} is used to investigate the influence of the regularization parameter on total variation models \citeO{Langer:18} and has been utilized for selecting a locally adjusted regularization parameter in the $L^1$-$L^2$-TV model \citeO{Langer:19}.

A completely different approach has been pursued in \citeO{HiRaWuLa:17}, where the problem of finding a suitable locally varying regularization parameter in the multi-scale $L^2$-TV model is formulated as a bilevel optimisation problem. The objective function of the first level is constituted by indicators telling the parameter how to change, while the second level consists of the pre-dual of the multi-scale $L^2$-TV model. In this setting this optimisation problem simultaneously recovers the image as well as determines a spatially varying regularization weight. Details on the algorithm, analysis and numerical experiments can be found in \citeO{HiRaWuLa:17}. 

% However, our approach in \citeO{Langer:17}  differs significantly from these previous works, where problem \eqref{L2TVmultimodel2} is considered and Uzawa's method or an Uzawa-like method is utilized for the update of the spatially varying parameter. Note, that in Uzawa's method an additional parameter has to be introduced and chosen accordingly. In \citeO{Langer:17} an update-scheme of $\alpha$ is proposed which does not need any additional parameter and hence is not similar to Uzawa's method. Moreover, differently to the approaches in \cite{DonHinRin,HinRin} where the initial regularization parameter $\lambda>0$ has to be set sufficiently small, in our approach any initial $\alpha>0$ is allowed. In this sense is our algorithm even more general than the ones presented in \cite{DonHinRin,HinRin}.


\subsubsection{Finite element discretization}

While in all the aforementioned works for discretization the finite difference method (FDM) is used, in \citeO{AlkamperLanger:17} a finite element discretization of the $L^1$-$L^2$-TV model with $T_1=T_2=I$ has been presented. In particular the finite element discretization is considered on conforming and locally adaptive meshes. Analytically we have proven that a minimizer of the treated model on a suitable continuous finite element space indeed converges to a minimizer of the original problem in the space of bounded variation functions. In order to obtain a solution of the considered optimisation problem, we have adapted the primal-dual algorithm of \cite{ChaPocFEM}  and have implemented it in the programming environment \dune \cite{Blaetal}. Further, the implementation is based on an adaptive refinement of the mesh, where the refinement is performed heuristically at known discontinuities. This demonstrates that an efficient implementation of nonsmooth total variation minimization problems in a FE framework is indeed possible. 

A comparison of continuous and discontinuous finite element spaces has been performed in the master thesis \citeO{Dieterich:19} supervised by the principle investigator Andreas Langer. In the situation of noise-free images and the $L^2$-TV model with $T=I$ it has been demonstrated that a discontinuous finite element space allows to preserve edges much more accurate than a continuous space.

%\textcolor{blue}{Here should go (Andreas): the M.Sc. thesis of A. Dieterich; all the own work on (local adaptive) parameter selection; FE discretization of \eqref{L1L2TVmodel} in DUNE; }

\textcolor{blue}{Here should go (Fernando): Any work on a posteriori estimates, AFEM, ... (about 1 page)}
\todo{to be done!!!!!!!!!!}





%\small 
\makeatletter
%\renewcommand{\@biblabel}[1]{[\textsf{O}.#1]}
\renewcommand*\bib@heading{%
\subsection{Project-related publications \textcolor{green}{(0.75 pages)}}%
}
\makeatother
{\small
\bibliographystyleO{abbrv}
\bibliographyO{nmh}
}
%\bibliographystyle{acm}
%\begin{thebibliography}{O10}
%
%\itemsep=-3.5pt
%\ourbibitem{AlkLan}
%{\sc Alk\"amper, M. and Langer, A.}
%\newblock{Using DUNE-ACFem for non-smooth minimization of bounded variation functions}.
%\newblock{\em submitted to Archive of Numerical Software}, (2016), pp. 17.
%
%\ourbibitem{HinLan2013} 
%{\sc Hinterm\"uller, M., and Langer, A.}
%\newblock {Subspace correction methods for a class of nonsmooth and nonadditive convex variational problems with mixed $L^1/L^2$ data-fidelity in image processing}.
%\newblock {\em SIAM J. Imaging Sciences 6}, 4 (2013), 2134--2173.
%
%\ourbibitem{Lan2016} 
%{\sc Langer A.}
%\newblock {Automated parameter selection in the $L^1$-$L^2$-TV model for removing Gaussian plus impulse noise}.
%\newblock { accepted by {\em Inverse Problems}}, (2016), pp. 41.
%%\itemsep=-3.5pt
%%\ourbibitem{O:GaHeSi:14}
%%{\sc Gaspoz, F.~D., Heine, C.-J., and Siebert, K.~G.}
%%\newblock Optimal grading of the newest vertex bisection and ${H}^1$-stability of
%%  the ${L}_2$-projection.
%%\newblock Stuttgarter Mathematische Berichte 2014-015 
%%(\url{http://www.mathematik.uni-stuttgart.de/preprints/downloads/2014/2014-015.pdf}),
%%2014. Submitted for publication to {\em IMA J. of Numer. Anal.}
%%
%%\ourbibitem{O:GaspozMorin:14}
%%{\sc Gaspoz, F.\,D.  and  Morin, P.}
%%\newblock Approximation classes for adaptive higher order finite element
%%  approximation.
%%\newblock {\em Math. Comp.} 83(289):2127-2160, 2014. 
%%
%%\ourbibitem{O:KoRoSi:14}
%% {\sc Kohls, K.,  R\"osch, A. and  Siebert, K.\,G.}
%%\newblock A Posteriori error analysis of optimal control problems with control constraints.
%%\newblock {\em SIAM J. Control Optim.}, 52(3):1832--1861, 2014.
%%
%%\ourbibitem{O:KrMoScSi:12}
%% {\sc Kreuzer, C.,  M\"oller, C.\,A.,  Schmidt, A. and Siebert, K.\,G. }
%%\newblock Design and convergence analysis for an adaptive discretization of the
%%  heat equation.
%%\newblock {\em IMA J. Numer. Anal.} 32(4):1375-1403, 2012.
%%
%%\ourbibitem{O:KreuzerSiebert:11}
%%{\sc Kreuzer, C. and  Siebert, K.\,G.}
%%\newblock Decay rates of adaptive finite elements with {D}\"orfler {M}arking.
%%\newblock {\em Numer. Math.}, 117(4):679--716, 2011.
%%
%%\ourbibitem{O:Siebert:11}
%% {\sc Siebert, K.\,G.}
%%\newblock A convergence proof for adaptive finite elements without lower bound.
%%\newblock {\em IMA J. of Numer. Anal.}, 31(3):947--970, 2011.
%\end{thebibliography}
\normalsize

\section{Objectives and work programme} 
\label{sec:object}
The main objective of the proposed research project is the enhancement of the restoration of noisy images. This enhancement of the restoration quality is achieved by developing adaptive methods for solving total variation minimization problems. The adaptivity in the method is designed in a way that features and edges in the considered image are preserved while noise is substantially eliminated. Such an adaptivity leads to a locally dependent treatment of the underlying problem, i.e., at different points or areas in the image domain the considered problem will be treated differently. The developed method will be adaptive in two ways. More precisely, the method allows for a local mesh refinement such that a locally adjusted regularization parameter can be optimally tailored to edges and transition of colors. We will create local adjustment methods with the properties of being robust with respect to the observed data and efficient. By efficient we mean, that the methods are also optimized with respect to computational time, why an adaptive local refinement rather than a global refinement is performed. The development of the method will be accompanied by a rigorous mathematical analysis, investigating the obtained discrete problems as well as the obtained algorithms theoretically. The designed method will be explored and tested by numerous numerical experiments considering applications in image denoising.
%The main objective of the proposed research project is the development and analysis of adaptive methods for solving inverse imaging problems, where the total variation minimization plays a fundamental role. The adaptivity in the methods is designed in a way that the reconstruction error is minimized during an iterative scheme or the computational complexity is reduced, while aiming at a certain accuracy of the solution, which may be obtained by optimising simultaneously the degrees of freedom and the reconstruction error. Such an adaptivity leads to a locally dependent treatment of the underlying problem, i.e., at different points or areas the considered problem will be treated differently. We will create local adjustment methods with the properties of being robust with respect to the observed data and efficient. By efficient we mean, that the methods are optimized with respect to computational time while keeping the numerical solution at a certain accuracy. % with the aim of reducing the reconstruction error. %Since we are dealing with inverse problems, this could mean that at different locations different regularization is used. 
%
 % non-smooth variational image reconstruction including total variation minimization. 
%  The designed methods will be explored and tested by numerous numerical experiments considering applications the applications of determining the optical flow of image sequences %, i.e., the determination of the optical flow, 
%  and image inpainting. %In the course of the project we \newf{will} mainly focus on the $L^1$-$L^2$-TV model, see \eqref{L1L2TVmodel}, which is a generalization of standard image models and is able to treat images that are corrupted by a mixture of noise or unknown noise.


\subsection{Anticipated total duration of the project}
\label{sec:duration}

3 years

\subsection{Objectives}\label{sec:Objectives}
%\todo{Objective genauer auf den Punkt ausarbeiten!}

The aim of the current project is to improve state-of-the-art locally adaptive parameter selection strategies for total variation image denoising. %By image denoising tasks we mean that the observed image contains noise and might be additionally corrupted by further instances, like blur or subsampling. That is the observation $g$ is given as in \eqref{eq:g}, where the operator $T$ might be, for example, the identity (solely denoising), a blurring operator (deblurring + denoising), a subsampled Fourier-transform (reconstruction of CT images), or a subsampled wavelet-transform.\todo{what type of noise?!}
For image denoising it has been demonstrated multiple times that a locally depend regularisation parameter is able to produce better results than a globally constant parameter \cite{}. In particular the local regularisation parameter should be large in homogeneous regions, while in parts with details the parameter should be small in order to preserve the structures. When looking at a concrete digital image, the underlying analogue image is usually discretized by quadratic pixels of uniform size. For further image enhancement, e.g. removing noise, this strongly supplies a finite difference (FD) implementation of image restoration algorithms. Considering a FD discretization, a local parameter may be viewed as a piecewise constant function, being constant on each pixel. That is, any constant part of the parameter is at least of the size of a pixel. This is (clearly) a restriction of the spatial adjustment of the parameter which may have influence on the restoration; cf. Figure~\ref{fig:1Dsignal}.

\begin{figure}[h!]
\begin{center}
\subfigure[\label{fig:noisy_signal}Observed noisy signal]{\includegraphics[width=5cm]{observed_signal}}\hspace{0.2cm}
\subfigure[\label{fig:reconstr}Reconstructions versus original signal]{\includegraphics[width=5cm]{Comparison_uniform_nonuniform_mesh}}\hspace{0.2cm}
\subfigure[\label{fig:zoom}Zoom of \ref{fig:reconstr}]{\includegraphics[width=5cm]{Comparison_uniform_nonuniform_mesh_zoom}}
\caption{Influence of h-adaptivity for reconstruction of a noisy signal, see \ref{fig:noisy_signal},  with a locally adjusted parameter.}\label{fig:1Dsignal}
\end{center}
\end{figure}


In order to overcome this constraint, in this project we develop methods, which combine a local adaptive parameter selection with a local refinement ($h$-adaptivity). Of course one may refine globally uniformly. However this would lead very fast to a very high computational complexity possibly not even manageable by a standard computer. For example, consider a medium size image of $1024 \times 1024$ pixels ($1\ 048\ 576$ pixels). Only five refinements, where a refinement quarters a pixel uniformly, would lead to an image which has $2^{10}$ times more elements, i.e., $1\ 073\ 741\ 824$ pixels. Hence it is not advised to use global refinement at all, and motivates us to consider local refinement, which allows to keep the degrees of freedom at a manageable size. Note that a (local) refinement alone does not improve the solution of the optimisation problem for a given regularization parameter. Actually it is the regularization parameter which decides how much smoothing is performed in the solution process. Hence in order to get sharp edges and preserve fine details the regularization parameter has to be locally adjusted. 
%This can be easily seen by the following observation on inverse problems: Let $X,Y$ be two normed vector spaces. Given $A\in \mathcal{L}(X,Y)$ and $y^\delta\in Y$, where $\|y-y^\delta\|_Y \leq \delta$ and $y\in Y$, we are seeking for a solution $x\in X$ such that $Ax=y$, whereas the equation is ill-posed. Let $(R_{\alpha})_\alpha$ be a regularisation of the pseudoinverse $A^\dagger$ of $A$ and $D_h: Y \to Y_h \subset Y$ a discretization. Then we have 
%$$
%\|R_\alpha D_h y^\delta - A^{\dagger} y\|_X \leq \underbrace{\|R_\alpha D_h (y^\delta - y)\|_X}_{\text{data error}} + \underbrace{\|R_\alpha D_h y - R_\alpha y\|_X}_{\text{discretisation error}} + \underbrace{\|R_\alpha y - A^{\dagger} y \|_X}_{\text{regularisation error}}.
%$$
%The sum of the last two terms is the so-called approximation error. It is well-know that for a fixed $\delta$ the data error and the approximation error propagate opposite for $\alpha,h \to 0$, see Figure \ref{}. Let $D_{\overline{h}}$ be another discretization such that $D_{\overline{h}}$ and $D_{h}$ are not necessarily nested and we use the convention that $D_hD_{\overline{h}} = D_h$. If $y = D_{\overline{h}} y$, then the minimal discretisation error is obtained for $D_h$ being such that $D_h y = D_{\overline{h}}y$, which is true if $D_h = D_{\overline{h}}$ or $D_h$ being a nested refinement of $D_{\overline{h}}$. %If $y = D_{\overline{h}} y$ is discrete and we use the convention that $D_hD_{\overline{h}} = D_h$, then $D_h = D_{\overline{h}}$ would give the minimal discretisation error. 

Let us motivate our approach further on by demonstrating with a simple one-dimensional example the strength of a combined local refinement and local parameter selection. We consider the noisy signal in Figure~\ref{fig:noisy_signal}, where the noise is of Gaussian type, and restore the original signal once on the original (observed) mesh, i.e., mesh-size $h=0.01$, with a locally adjusted parameter and once on a locally twice refined mesh around the discontinuity again with a locally adjusted parameter by minimizing \eqref{L2TVmultimodel2}. For the concrete choices of the local refined mesh and locally adjusted parameters see Figure~\ref{fig:stepsize} and Figure~\ref{fig:lamdba} respectively. That is, the non-uniform (locally refined) case allows for a more precise choice of the regularization parameter leading to a significantly more accurate restoration than the uniform setting, see Figure~\ref{fig:1Dsignal}.
%In order to motivate our approach and demonstrate the strength of a combined local refinement and local parameter selection we depict in 
%In Figure~\ref{fig:1Dsignal} we depict for a one-dimensional signal that a local refinement together with a local parameter selection indeed allows for better reconstruction. The mesh-size in Figure~\ref{fig:1Dsignal} is chosen to be $h=0.01$ in the uniform case and in the case of a non-uniform mesh the grid is two times refined around the jump, see Figure \ref{fig:stepsize}.  According to the grid the parameter $\lambda$ is locally adjusted as depicted in Figure \ref{fig:lamdba}. That is, in the non-uniform case this allows for a more precise choice of the regularization parameter leading to a significantly more accurate restoration than in the uniform setting, see Figure~\ref{fig:1Dsignal}.
 Note that in this motivative example the mesh refinement as well as the parameter choice of $\lambda$ is set by hand knowing the position of the discontinuity in advance. 
 
 The goal of this project is to develop a method which fully automatic adapts the grid as well as the regularization parameter in total variation image denoising problems without a priori knowing the places of the discontinuities (free-discontinuity problem) in order to increase the restoration quality significantly. More precisely the developed method will have the following desirable properties: 
\begin{inparaenum}[(i)]
\item it obtains restorations of higher quality than state of the art methods;
\item it is computationally efficient in time;
\item it is stable and robust.
\end{inparaenum}
\begin{figure}
\begin{center}
\includegraphics[width=5cm]{stepsize}
\caption{Stepsize of the non-unform mesh. Twice refined around the jump.}\label{fig:stepsize}
\end{center}
\end{figure}
\begin{figure}
\begin{center}
\subfigure[\label{fig:lambda1}]{\includegraphics[width=5cm]{lambda}}\hspace{0.2cm}
\subfigure[\label{fig:lambda2}]{\includegraphics[width=5cm]{lambda_zoomed}}
\caption{Choice of the locally adaptive parameter}\label{fig:lamdba}
\end{center}
\end{figure}
The goal is achieved by addressing the following challenges:
\begin{itemize}
\item At which image parts should be locally refined and how should this refinement be performed?
\item What is a suitable discretization method (FD, FE) for h-adaptive meshes in image denoising?
\item How to combine h-adaptivity and locally adaptive parameter selection strategies? 
\item What are the theoretical requirements to ensure convergence, robustness, and stability of the developed locally adaptive method?
\end{itemize}
Answering these research questions by means of the below described work packages (see next subsection) will lead to substantial scientific novelties. Concrete contributions of this project include the development of an algorithm which fully automatically adjusts locally the regularization parameter and locally refines the mesh for image denoising. In particular, to the best of our knowledge, this is the first mathematical contribution to simultaneously perform h-adaptivity and local adaptive parameter selection for an image restoration task.

Our method will be tailored to concrete applications of total variation image denoising tasks. By image denoising tasks we mean that the observed image contains noise and might be additionally corrupted by further instances, like blur or subsampling. That is the observation $g$ is given as in \eqref{eq:g}, where the operator $T$ might be, for example, the identity (solely denoising), a blurring operator (deblurring + denoising), a subsampled Fourier-transform (reconstruction of CT images), or a subsampled wavelet-transform, and $\mathcal{N}$ represents Gaussian noise, impulsive noise or a combination of both, leading to optimization problems of the form \eqref{eq:L1L2TV}. % \todo{what type of noise?!}
In practice exactly such situations need to be dealt with, e.g., in medical imaging and in the restoration of stored and transmitted photographs. Hence our proposed method will be able to improve the restoration capability of several in practice very relevant and important imaging tasks.
%challenging and very important restoration tasks appearing, Hence our method will be able to improve the restoration capability of several very relevant imaging tasks.

%By image denoising tasks we mean that the observed image $g$ contains noise, but might be additionally corrupted by blur or subsampling. That is the operator $T$ in \eqref{L2TV} might be, for example, the identity (solely denoising), a blurring operator (deblurring + denoising), a subsampled Fourier-transform (reconstruction of CT images), or a subsampled wavelet-transform. 

%Although our considerations are restricted to total variation regularization, it will serve as an important groundwork
%Although our method is tailored to the concrete application of image denoising tasks, it will serve as an important groundwork for other applications, such as image inpainting, which is considered in a later phase of the project. 


%The goal of the project is achieved by developing and implementing an adaptive method which automatically adjusts to the underlying problem, respectively image. The adjustment is, on the one hand side, happening on the discretization-level, where an optimal adaptive discretization is learned


%The aim of the current project is to derive, analyse and implement methods for non-smooth optimization problems containing a total variation term with applications in image inpainting and motion estimation, which automatically adjust to the underlying problem. This adjustment, on the one hand-side, may happen on the modelling side where the according objective functional, e.g., its regularization term and parameters, is automatically chosen, and on the other hand-side, may occur on the discretization-level, where an optimal discretization is learned. This two-sided adjustment is motivated by the following observation on inverse problems: Let $X,Y$ be two normed vector spaces. Given $A\in \mathcal{L}(X,Y)$ and $y^\delta\in Y$, where $\|y-y^\delta\|_Y \leq \delta$ and $y\in Y$, we are seeking for a solution $x\in X$ such that $Ax=y$, whereas the equation is ill-posed. Let $(R_{\alpha})_\alpha$ be a regularisation of the pseudoinverse $A^\dagger$ of $A$ and $D_h: Y \to Y_h \subset Y$ a discretisation. Then we have 
%$$
%\|R_\alpha D_h y^\delta - A^{\dagger} y\|_X \leq \underbrace{\|R_\alpha D_h (y^\delta - y)\|_X}_{\text{data error}} + \underbrace{\|R_\alpha D_h y - R_\alpha y\|_X}_{\text{discretisation error}} + \underbrace{\|R_\alpha y - A^{\dagger} y \|_X}_{\text{regularisation error}}.
%$$
%The sum of the last two terms is the so-called approximation error. It is well-know that for a fixed $\delta$ the data error and the approximation error propagate opposite for $\alpha,h \to 0$, see Figure \ref{}. \nnew{Let $D_{\overline{h}}$ be another discretization such that $D_{\overline{h}}$ and $D_{h}$ are not necessarily nested and we use the convention that $D_hD_{\overline{h}} = D_h$. If $y = D_{\overline{h}} y$, then the minimal discretisation error is obtained for $D_h$ being such that $D_h y = D_{\overline{h}}y$, which is true if $D_h = D_{\overline{h}}$ or $D_h$ being a nested refinement of $D_{\overline{h}}$.} %If $y = D_{\overline{h}} y$ is discrete and we use the convention that $D_hD_{\overline{h}} = D_h$, then $D_h = D_{\overline{h}}$ would give the minimal discretisation error. 
%On the contrary, assuming uniform meshes with mesh-size $h<\overline{h}$, the data error increases, and for $h>\overline{h}$ the data error decreases. Hence, in the noise-free case, i.e. $\delta=0$, from this consideration we deduce that $D_h=D_{\overline{h}}$ (resp. $D_h$ such that $D_h y = D_{\overline{h}} y$) seems to be the best choice. However, for $\delta>0$, %, which is indeed the case for $\delta=0$, i.e., in the noise-free situation. But for $\delta>0$, 
%since the data error may be reduced for $h > \overline{h}$, the choice of the discretisation seems not clear. %From this consideration it seems not clear that $D_h=D_{\overline{h}}$ is the best choice, as it depends on how the data error may be reduced for $h > \overline{h}$. 
%%Nevertheless, numerical experiments suggest to take $D_h=D_{\overline{h}}$ \cite{}, while a reason for this might be, that it is very difficult to optimise a regularisation method with respect to a possibly non-uniform mesh. 
%In this project we mainly concentrate at looking at the considered inverse imaging problems from the perspective that the objects depicted in an image are analog. %Nevertheless, investigating the influence of a discretisation on the data error for given discrete observations should be part of .... \todo{see if it fits somewhere}. 
%Hence, here we mainly assume $y$ being a function rather than a discrete object. For example, then in image inpainting we are interested in restoring the corrupted physical objects depicted in the image as accurate as possible by keeping the computational complexity on a manageable level. This accurate restoration may also be put in the context of super-resolution or zooming.
%
%We derive criteria which tell us how the adjustment of the regularization and discretization should be performed. Since we are interested in imaging problems, an important criteria will be an indicator for discontinuities or edges. Exactly at discontinuities the regularization should be very little, preventing any smoothing. This asks for localized regularization and locally adaptive regularization parameters. Moreover, according to the discontinuities also the discretization should be aligned, e.g., in order to use a (discontinuous) Galerkin method successfully. Such an automatic alignment may be performed by solving an optimisation problem with respect to the discrete mesh or triangulation. However, in general such adjustment criteria will depend on the application. For example, if we calculate the optical flow of a (noise-free) image sequence, then an edge indicator of the flow field should be sufficient. Thereby, note, that the derivation of an accurate edge indicator is by far not a trivial task and we need to investigate what might be a good choice. However, assuming we have an edge indicator and if there is additional noise present, then such an indicator may detect noise as small features, which leads to false edge indications. In such a situation additional information has to be used, as the according noise-level, to construct a reasonable indicator. 
%
%The final goal of the present project is to provide a fully adaptive method for image inpainting and motion estimation, which automatically adjusts the regularisation and discretization accordingly. Moreover, strongly related problems, as the calculation of crack propagation, might be considered as well.




\subsection{Work programme incl. proposed research methods}
\label{sec:work-programme}

%The work program starts by deriving indicators, telling where and how to adaptively adjust the model and solution process. Based on these derived indicators, we construct different adaptive methods, which iteratively reduce the reconstruction error. The adaptivity may refer to the adaptive choice of the parameters in the objective functional (locally distributed parameters), the adaptive choice of stopping criteria in an iterative regularization method (iterative adaptive regularization), or the adaptive adjustment of the (discrete) mesh on which the problem should be solved (adaptive discretization). The work program of the proposed project is separated into three parts, while in all parts we mainly focus on applications to image inpainting and motion estimation. In the first part we develop and analyse adaptive discretization schemes, while in the second part adaptive regularization methods are constructed and investigated. In the third part the implementation of the adaptive schemes is considered, where we aim at a fully automatic adjustment scheme with respect to discretization and regularization. The first two parts are planned in a way that they benefit from each other. %might be developed independently from each other. However, the work programme is planned in a way, that the second part benefits form the first one and maybe vice-versa. 
%For example, indicators, based on a posteriori estimates, for an automatic adjustment of the discretization, may also be used to adjust the regularization accordingly and vice-versa. \nnew{Note that (an adaptive) refinement alone only improves the solution of the optimisation problem for a given regularization parameter. Actually it is the regularization parameter which decides how much smoothing is performed in the solution process. Hence in order to get sharp edges and preserve fine details the regularization parameter has to be locally adjusted.} %Additionally,  an adaptive discretization alone, might not improve the solution but only reduces the degree of freedom. 
%\nnew{That is} in order to gain significant improvement in the reconstruction, we need to combine adaptive discretization with a locally adaptive parameter choice. %\nnew{For example, in image inpainting a fine discretization in the inpainting region is not sufficient to obtain a more accurate reconstruction} \todo{maybe example inpainting where both is needed}

 




%\subsubsection{Part I: Combining $h$-adaptivity and local adaptive parameter selection}\label{Bereich1}

%\paragraph{Goals}
%
%For image denoising it has been demonstrated multiple times that a locally depend regularisation parameter is able to produce better results than a globally constant parameter \cite{}. In particular the local regularisation parameter should be large in homogeneous regions, while in parts with details the parameter should be small in order to preserve the structures. When looking at a concrete digital image, the underlying analogue image is usually discretized by quadratic pixels of uniform size. For further image enhancement, e.g. removing noise, this strongly supplies a finite difference (FD) implementation of image restoration algorithms. Considering a FD discretization, a local parameter may be viewed as a piecewise constant function, being constant on each pixel. That is, any constant part of the parameter is at least of the size of a pixel. This is (clearly) a restriction of the spatial adjustment of the parameter which may have influence on the restoration; cf. Figure~\ref{fig:1Dsignal}.
%
%\begin{figure}[h!]
%\begin{center}
%\subfigure[\label{fig:noisy_signal}Observed noisy signal]{\includegraphics[width=5cm]{observed_signal}}\hspace{0.2cm}
%\subfigure[\label{fig:reconstr}Reconstructions versus original signal]{\includegraphics[width=5cm]{Comparison_uniform_nonuniform_mesh}}\hspace{0.2cm}
%\subfigure[\label{fig:zoom}Zoom of \ref{fig:reconstr}]{\includegraphics[width=5cm]{Comparison_uniform_nonuniform_mesh_zoom}}
%\caption{Influence of h-adaptivity for reconstruction of a noisy signal, see \ref{fig:noisy_signal},  with a locally adjusted parameter.}\label{fig:1Dsignal}
%\end{center}
%\end{figure}
%
%
%In order to overcome this constraint, in this part of the project we develop methods, which combine a local adaptive parameter selection with a local refinement ($h$-adaptivity). Of course one may refine globally uniformly. However this would lead very fast to a very high computational complexity possibly not even manageable by a standard computer. For example, consider a medium size image of $1024 \times 1024$ pixels ($1\ 048\ 576$ pixels). Only five refinements, where a refinement quarters a pixel uniformly, would lead to an image which has $2^{10}$ times more elements, i.e., $1\ 073\ 741\ 824$ pixels. Hence it is not advised to use global refinement at all, and motivates us to consider local refinement, which allows to keep the degrees of freedom at a manageable size.
%
%Let us motivate our approach further on by demonstrating with a simple one-dimensional example the strength of a combined local refinement and local parameter selection. We consider the noisy signal in Figure~\ref{fig:noisy_signal} and restore the original signal once on the original (observed) mesh, i.e., mesh-size $h=0.01$, with a locally adjusted parameter and once on a locally twice refined mesh around the discontinuity again with a locally adjusted parameter. For the concrete choices of the local refined mesh and locally adjusted parameters see Figure~\ref{fig:stepsize} and Figure~\ref{fig:lamdba} respectively. That is, the non-uniform (locally refined) case allows for a more precise choice of the regularization parameter leading to a significantly more accurate restoration than the uniform setting, see Figure~\ref{fig:1Dsignal}.
%%In order to motivate our approach and demonstrate the strength of a combined local refinement and local parameter selection we depict in 
%%In Figure~\ref{fig:1Dsignal} we depict for a one-dimensional signal that a local refinement together with a local parameter selection indeed allows for better reconstruction. The mesh-size in Figure~\ref{fig:1Dsignal} is chosen to be $h=0.01$ in the uniform case and in the case of a non-uniform mesh the grid is two times refined around the jump, see Figure \ref{fig:stepsize}.  According to the grid the parameter $\lambda$ is locally adjusted as depicted in Figure \ref{fig:lamdba}. That is, in the non-uniform case this allows for a more precise choice of the regularization parameter leading to a significantly more accurate restoration than in the uniform setting, see Figure~\ref{fig:1Dsignal}.
% Note that in this motivative example the mesh refinement as well as the parameter choice of $\lambda$ is set by hand knowing the position of the discontinuity in advance. The goal of this project is to develop a method which fully automatic adapts the grid as well as the regularization parameter in total variation image denoising problems without knowing the places of the discontinuities (free-discontinuity problem) in order to increase the restoration quality significantly. More precisely the developed method will have the following desirable properties: 
%\begin{inparaenum}[(i)]
%\item it obtains restorations of higher quality than state of the art methods;
%\item it is computationally efficient in time;
%\item it is stable and robust.
%\end{inparaenum}
%\begin{figure}
%\begin{center}
%\includegraphics[width=5cm]{stepsize}
%\caption{Stepsize of the non-unform mesh. Twice refined around the jump.}\label{fig:stepsize}
%\end{center}
%\end{figure}
%\begin{figure}
%\begin{center}
%\subfigure[\label{fig:lambda1}]{\includegraphics[width=5cm]{lambda}}\hspace{0.2cm}
%\subfigure[\label{fig:lambda2}]{\includegraphics[width=5cm]{lambda_zoomed}}
%\caption{Choice of the locally adaptive parameter}\label{fig:lamdba}
%\end{center}
%\end{figure}
%The goal is achieved by addressing the following challenges:
%\begin{itemize}
%\item At which image parts should be locally refined and how should this refinement be performed?
%\item What is a suitable discretization method (FD, FE) for h-adaptive meshes in image denoising?
%\item How to combine h-adaptivity and locally adaptive parameter selection strategies? 
%\item What are the theoretical requirements to ensure convergence, robustness, and stability of the developed locally adaptive method?
%\end{itemize}
%Answering these research questions by means of the below described work packages (see next subsection) will lead to substantial scientific novelties. Concrete contributions of this project include the development of an algorithm which fully automatically adjusts locally the regularization parameter and refines the mesh for image denoising. In particular, to the best of our knowledge, this is the first mathematical contribution to simultaneously perform h-adaptivity and local adaptive parameter selection for an image restoration task.
%
%Our method will be tailored to the concrete application of total variation image denoising tasks, whereby we mean that the observed image contains noise, but might be additionally corrupted by blur or subsampling. That is the operator $T$ in \eqref{L2TV} might be, for example, the identity (solely denoising), a blurring operator (deblurring + denoising), a subsampled Fourier-transform (reconstruction of CT images), or a subsampled wavelet-transform. These are indeed in practice challenging and very important restoration tasks appearing, e.g., in medical imaging and in the restoration of stored and transmitted photographs. Hence our method will be able to improve the restoration capability of several very relevant imaging tasks.
%
%%Although our considerations are restricted to total variation regularization, it will serve as an important groundwork
%%Although our method is tailored to the concrete application of image denoising tasks, it will serve as an important groundwork for other applications, such as image inpainting, which is considered in a later phase of the project. 

%\paragraph{Approach and Work Programme}
 
 Based on the goals and challenges defined above, we separate the work programme into the following work packages (WP).
 
 
\subsubsection*{WP 1: Indicator}
In this part indicators telling in which locations of the image domain the grid should be refined or coarsened and how the locally varying regularization parameter should be adjusted are derived. We start by constructing heuristic indicators based on the idea that the mesh should be finer around edges than in relative uniform areas. At a later stage we also derive fully analytic motivated criteria based on error estimates.
 
 \subparagraph{WP 1.1: Heuristic indicator for $h$-adaptivity} We derive criteria which tell us at which parts of the image domain the mesh needs/should be further refined. In order to allow the PhD candidate to get easier familiar with the topic, in the beginning such a criterion might be heuristic, motivated by intuitive considerations or numerical observations. Let us describe two possible approaches:
\begin{enumerate}
 \item As already stated above, we wish the regularization parameter to be locally small at discontinuities, in order to preserve them. Hence it seems suitable that the mesh is fine around a discontinuity allowing to localize the jump accurately. In this vein, a possible indicator may be an edge indicator, as the Sobel operator or the Canny detector. A consequential strategy to obtain a suitable refinement indicator might be \begin{inparaenum}[(i)] \item (pre-)denoise the image by a standard technique \item apply an edge detector \item define the indicator according to the result of the edge detector\end{inparaenum}.
This refinement indicator is then combined with a locally adaptive parameter selection strategy. As there exist several different locally adaptive parameter selection strategies, we may use one of these; see \cite{}, and compare them numerically.  \\ %first use the one described in \cite{Lan}. Subsequently we will adjust the approach to other adaptive parameter selection strategies, as \cite{}, and compare them numerically.  \\
\textcolor{blue}{(Additional comments: ad (i) How accurate does the pre-denoised solution need to be? Should it be over-smoothed (no noise but possibly features and edges are lost) or under-smoothed (still noisy but no features lost)? Which method is suitable (filter, like Gaussian filter, or an variational approach, as TV-denoising)? ad (ii) How accurate should the edge detector be? Is it preferable to use a reluctant one or should it detect nearly everything, which would also include noise? All these question lead to a question of philosophy: Should be refined in case of doubt, or not? From a quality point of view we should refine better more than too less, as this allows for a more precise adjustment of the regularization parameter. However this might be computationally too costly. This needs to be checked and acted accordingly!)}

\item As we are looking for a combined h-adaptive and locally adaptive parameter choice method, we may first select a locally adaptive parameter choice method, e.g., \cite{DonHinRin}. This method then already delivers an indicator for adjusting the parameters locally. This indicator will be investigated how it can be utilized to perform h-adaptivity. More precisely we believe that the indicator provided in \cite{DonHinRin} for the adjustment of the parameter can also be used for indicating h-adaptivity, as this indicator leads to parameters which regularize less around features while regularizing much in homogeneous regions, see \cite{DonHinRin}. Subsequently this approach is then also adjusted to other locally adaptive parameter selection methods and accompanied by a numerical comparison.\\
\textcolor{blue}{(Additional comment: The local adaptive parameter choice is an iterative procedure. Such an indicator usually depends on the previously computed approximation and hence depends on the regularisation parameter. One has to be careful, as a small regularisation parameter leads to a global change of the parameter although locally differently. This needs to be taken into account.)}
\end{enumerate}  

In this context we also investigate how to transform the observed image on the refined grid. That is, is it enough to separate the pixels accordingly and keep the values as in the ``parent'' pixel or do we need a sophisticated super-resolution method, as refining is related to zooming? What will be the effects of the different approaches?

In both above described approaches we have different possibilities on how to combine the two types of adaptivity. 
\begin{enumerate}
\item \textbf{Alternately:} Iteratively perform the local adaptive parameter selection method  and the h-adaptivity, where we may start with the h-adaptivity or the parameter selection (which gives us 2 options);
\item \textbf{Simultaneously:} As the local adaptive parameter selection methods are of iterative nature, we may incorporate the h-adaptivity into the parameter selection iterations. This means, performing one or a few iterations of the parameter selection method before applying the h-adaptivity.  \textcolor{blue}{Formulate the problem as a bi-level optimisation problem (machine learning). Seems hard and would be a project on its own!}
\end{enumerate}
It will be investigated by numerical experiments which of these strategies is more suitable in practice for image denoising.

Note that such an indicator might not only be suitable for local refinement but also for local coarsening, in order to reduce the computational complexity even more. Such a coarsening is suitable in image parts, where the solution is intended to be homogeneous and thus the regularization parameter is uniformly large.
%In both approaches the two types of adaptivity are iterated following the scheme:
%\begin{enumerate}
%\item[step 1:] Perform the locally adaptive parameter choice method to a certain accuracy
%\item[step 2:] Perform h-adaptivity, i.e., adaptively refine mesh (continue with step 1 till a stopping criterion holds.)
%\end{enumerate} 
 
 % as already stated above, we wish the regularization parameter to be locally small at discontinuities, in order to preserve them. Hence it seems suitable that the mesh is fine around a discontinuity allowing to localize the jump accurately. In this vein, a possible indicator may be an edge indicator, as the Sobel operator or the Canny detector. A consequential strategy would be \begin{inparaenum}[(i)] \item pre-denoise the image by a fast standard technique, like a noise filter \item apply an edge indicator \item refine according to the indicator\end{inparaenum}.
 
%The h-adaptivity should be combined with a locally adaptive parameter selection strategy. 
 
 
 \subparagraph{WP 1.2: Theoretical motivated indicators}
 
  %At a later stage of the overall project 
  At this part an indicator for the $h$-adaptivity should be derived rigorously by mathematical argumentation based on a posteriori estimates. Note that there exists a variety of reliable and efficient estimators for a large class of different elliptic problems; compare with the monographs~\cite{AinOde,Ciarlet:02,Verfurth:96} and the references therein. In these cases error estimation is in general a two-step procedure: in a first step one shows, by a continuous inf-sup condition and continuity of the associated bilinear form, equivalence of the true error to some negative norm of the residual. Then the non-computable negative norm of the residual is estimated by an equivalent (up to oscillation) but computable quantity in the second step. It is important to notice that the equivalence of the first step is equivalent to have the continuous inf-sup and at the same time continuity of the bilinear form with respect to the chosen spaces. Recently a posteriori estimators have been developed for solving constrained optimization problems \cite{KoRoSi:14}. However, for the inverse problem of analysing or reconstructing images from observed data a posteriori estimators are not yet used. The reason for this might be that in such problems one usually minimizes a non-smooth functional, which do not allow usual derivatives and renders the optimality conditions complicated. To our knowledge the only result in this direction is~\cite{Bar2015} for the $L^2$-TV model in which an abstract version of Repin's technique~\cite{Repin} is applied.
 
 We will derive a posteriori error estimators for non-smooth optimization problems of the type \eqref{L2TV} with $T\not= I$. A possible strategy might be the following: 
\begin{enumerate}[1.]
\item We derive an estimate of the form $\|u -v \| \leq J(v) - J(u)$, where $u\in V$ is the minimizer of the original primal problem $\inf_{u\in V} J(u)$, $v\in V$ is arbitrary, and $\|\cdot\|$ is a respective norm.
\item We determine a respective dual problem $\sup_{p\in W^*} J^*(p)$, where $W^*$ is an associated dual space. Note, that the dual problem is not unique and different possibilities might be considered. %Note, that by the Fenchel-duality principle we have $\inf_{u\in V} J(u)\geq \sup_{p\in W^*} J^*(p)$.
\item We note, that by the Fenchel-duality principle we have $\inf_{u\in V} J(u)\geq \sup_{p\in W^*} J^*(p)$ and consequently we obtain $\|u -u_h \| \leq J(u_h) - J^*(p_h)$, where $u$ minimizes  $\inf_{u\in V} J(u)$ and $u_h$ and $p_h$ are discrete solution of the discrete primal and dual problem, respectively. 
\end{enumerate} 

Instead of tackling estimates for the primal problem directly, we will consider also the respective dual problem. This seems more suitable for deriving a posteriori estimates than the primal one, as if $T$ is injective a dual problem of \eqref{L2TV} can be formulated as
$$
\min_{p\in H_0(\operatorname{div},\Omega)} ||| \operatorname{div} p + T^*g|||_{B} \quad \text{s.t.} \quad |p|\leq \lambda
$$
where $B=(T^*T)^{-1}$ and $||| w |||_B := \langle w, B w \rangle_{L^2(\Omega)}$. Hence, this is a  double obstacle problem and standard techniques may be adapted to derive a posteriori estimates. %While this is the dual of the infinite dimensional problem, we also plan to consider a dual formulation of a discrete primal formulation, which has a similar structure than the continuous one, see Herzog et al. \cite{}. \todo{explain Herzog approach more, e.g. advantages}

%In this strategy one has to be careful in choosing the discrete spaces of the respective discrete versions in such a way that they are subspaces of the continuous spaces. 
Note that it is well-known that especially at singularities the reconstruction error is relatively large. Hence, we expect that a suitable a posteriori estimate should provide a well working indicator. In particular, based on the a posteriori estimations, we will extract suitable indicators, which tell us where fine details are located and a further adjustment of the respective method is needed.

An indicator solely based on a posteriori estimations may only perform suitably in a noise-free application, as they may detect noise as details and may ask for a fine discretization around noise. However we investigate how crucial this behaviour is in the restoration process of noisy images. In case this behaviour has indeed critical consequences, further informations detecting the noise are needed, so that a reasonable adaptive discretization is guaranteed. Such information might be the noise-level of the underlying noise, and hence may also depend on the type of noise. Note that the local adjustment of the regularization parameter anyway needs such an additional information and should not be solely based on an a posteriori estimate, due to the reasons mentioned before. A strategy which we plan to investigate follows the ideas in \cite{Lan, Lan} assuming some statistical knowledge about the difference $\eta:= g- T \hat{u}$ and computing local residuals in order to check whether a local area of the reconstruction (still) contains noise or is noiseless. It has been demonstrated in \cite{Lan} that for the adjustment of the regularization parameter the statistical knowledge of $\eta$ should be even of local nature, since the noise-level usually differs locally significantly.\\
In this project we follow this idea, by investigating on how to estimate noise locally rather accurately. Additionally or alternatively, we combine the obtained noise-level with an a posteriori indicator, derived as above, to obtain stable indicators of fine details and edges in an image. These indicators are then used for h-adaptivity and the local adjustment of the parameter. We believe that a combination of these techniques allows to create stable locally adaptive methods.
 
%The derived indicators of this work package will be incorporated into the finite difference and finite element implementation of the fully adaptive methods of WP 2.2 and WP 2.3.
  
 
 
\subsubsection*{WP 2: Methods}
This part is devoted to the development and analysis of suitable discretization methods for the proposed fully adaptive scheme.  In particular we will consider finite difference and finite element methods including continuous and discontinuous Galerkin methods. Accordingly we separate this package into three sub-packages.

 
\subparagraph{WP 2.1: Implementation of an adaptive finite difference method}
 
 The literature indicates that for solving the inverse problem of reconstructing an image from its observed corrupted data usually finite differences are used. In this vein we first implement a finite difference method for total variation denoising algorithms which can handle locally refined meshes. In particular we implement different solution strategies for solving the $L^2$-TV, $L^1$-TV and $L^1$-$L^2$-TV model. % the derived double adaptive algorithm by an adaptive finite difference method.  
 To the best of our knowledge, this seems to be the first adaptive finite difference approach for total variation minimization. The implementation of these algorithms form the basis of the locally adaptive parameter selection and $h$-adaptive method derived in WP 1.1, whose implementation, analysis and validation will be carried out in WP 2.2. % and WP 2.3. 


 \subparagraph{WP 2.2: Implementation of fully adaptive methods by finite differences}
 
 Based on the adaptive finite difference implementations in WP 2.1, we will implement the different adaptive strategies derived in WP 1. In particular, first the strategies based on the heuristic indicators (WP 1.1) are considered, while in a later phase also the theoretical motivated indicators (WP 1.2) are implemented. Thereby the construction of the fully adaptive methods (i.e., adaptive with respect to the mesh and regularization parameter) is done such that the resulting algorithms converge, which should be theoretically justified. Note that the algorithms for the $L^2$-TV, $L^1$-TV  and $L^1$-$L^2$-TV models will be quite similar, as long as only one parameter is locally adjusted. In case of the $L^1$-$L^2$-TV model also two locally varying parameters are considerable, which needs a new derivation of an adaptive algorithm and consequently a theoretical investigation of the respective underlying optimization problem.  
 
 Although the main focus in this work package lays on the numerical implementation, we are interested in the development of theoretically sound algorithms. Hence further interesting theoretical aspects of the fully adaptive method, as robustness, stability and convergence order, will be investigated.
 
 We compare and numerically investigate the restoration capability as well as the computational performance of the derived numerical schemes. Moreover
 %
% These numerical investigations of the fully adaptive algorithm will be complemented by a mathematical analysis investigating the convergence and stability of the method. 
 %
% The literature indicates that for solving the inverse problem of reconstructing an image from its observed corrupted data usually finite differences are used. In this vein we first implement a finite difference method on locally refined meshes for total variation denoising algorithms. In particular we implement different solution strategies for solving the $L^2$-TV, $L^1$-TV and $L^1$-$L^2$-TV model. % the derived double adaptive algorithm by an adaptive finite difference method.  
 %To the best of our knowledge, this seems to be the first adaptive finite difference approach for total variation minimization. The implementation of these algorithms forms the base of the locally adaptive parameter selection and $h$-adaptive method derived in WP 1.2, whose implementation, analysis and validation will be explained in WP 1.3
 %
 %The derivation and implementation of the fully adaptive algorithm will be complemented by a mathematical analysis investigating the convergence and stability of the method. 
 %
 we extensively compare our derived adaptive algorithms with state-of-the-art algorithms for image denoising applications, described in WP 4.
 
 
% The numerical investigations of the fully adaptive algorithm will be complemented by a mathematical analysis investigating the convergence and stability of the method. 
 
 \subparagraph{WP 2.3: Finite element discretization}
 
 An alternative discretization (to FD) is given by finite element methods, which easily allow for an adaptive meshing. However finite element methods are not yet well established and understood in the context of non-smooth optimisation problems with applications in image processing. Nevertheless finite element methods in imaging applications are needed whenever the image is not represented on a Cartesian grid, for example when the image data is obtained by the use of hexagonal sensor layouts \cite{}, the data is given on manifolds, or in medical imaging \cite{}; see \cite{} for motivations considering finite element discretization of the total variation also in the context of other applications besides imaging. \todo{References from Herzog et al.} 
  Therefore we develop and investigate in more depth finite element methods for total variation minimization. We will examine whether it is more suitable to first discretized and then optimised the considered problem, as in \cite{Herzog etal}, or the other way round, i.e., first optimise and then discretized, as in \cite{AlkLan}. By suitable we mean, which approach is computational more efficient and has better restoration capabilities. %A comparison of both approaches in the sense of quality and efficiency will be carried out. 
  Such a comparison includes the implementation of different total variation minimization strategies, such as the Augmented Lagrange Method, ADMM, primal-dual method of Chambolle and Pock, dual methods, as well as considering different finite element spaces. \textcolor{blue}{A comparison of different methods in a FE setting could be announced as a master-thesis; same may apply for DG $\Rightarrow$ HiWi job.} Due to the fact that the underlying solution is supposed to be discontinuous, we expect a suitable finite element space to be discontinuous, leading to a discontinuous Galerkin (DG) implementation. In fact, it is demonstrated in the master-thesis \cite{Dieterich} (supervised by the principle investigator Andreas Langer) that a DG implementation -- in case of the symmetric interior penalty method -- is able to preserve edges tremendously more accurate than a continuous Galerkin (CG) implementation. A key point in the DG implementation is that the penalty term is made spatially adaptive, so that the formulation is only penalized largely where the solution should be continuous. Motivated by these positive results performed on uncorrupted test images, in this project we will investigate how a DG implementation may be used efficiently to solve total variation image denoising problems. 
  
  The obtained different discrete optimisation problems as well as the methods arising from the CG and DG formulations will be accompanied by a rigorous mathematical analysis, showing well-posedness,  $\Gamma$-convergence of the discrete formulation to the continuous problem, a priori error estimates and convergence of the derived schemes.
  
    %In particular in this situation the adaptivity will be threefold, i.e., the h-adaptivity, the locally adaptive regularization parameter, and the locally adaptive penalty term of the DG formulation. 
  After performing the investigations described above, we will have a good understanding of the pro and cons of the different implementations, allowing us to decide which one is most suitable for image denoising. 

Based on this considerations in a next step we will incorporate the h-adaptivity as well as the adaptive parameter selection into the finite element framework. In particular in the situation of a DG implementation the adaptivity is then threefold, i.e., h-adaptivity, locally adaptive regularization parameter, and locally adaptive penalty term of the DG formulation. Numerical experiments showing the efficiency of the proposed method will be performed on realistic images for applications described in WP 4. %the application of image denoising, whereby the operator $T$ in \eqref{L2TV} might not necessarily be the identity, but is for example also a blurring operator or a subsampled Fourier-transform.

\textcolor{blue}{(Describe software concept: DUNE; parallelisation; load balancing by space filling curves, ...)}

% Finite element methods in imaging applications are needed whenever the image is not represented on a Cartesian grid, for example when the image data is obtained by the use of hexagonal sensor layouts \cite{}, the data is given on manifolds, or in medical imaging \cite{}; see \cite{} for motivations considering finite element discretization of the total variation also in the context of other applications besides imaging. \todo{References from Herzog et al.} Moreover, the strength of finite element methods is, that they easily allow for an adaptive discretization, which reduces the degree of freedom of the problem to be solved and hence the computational complexity. We construct such an adaptive scheme based on an error estimator derived in Section \ref{Bereich1} (A). More precisely, a large local error indicates that the mesh needs to be further refined. On the contrary, a very small error allows to coarsen the mesh to reduce the dimensionality of the problem.
  
 

 
%How does the local adaptivity effect the solution? 
%
%Do the FE methods converge?
% 
% $\Gamma$-convergence; stability; a priori estimate; convergence w.r.t. h; robustness


 \subparagraph{WP 3: Theoretical analysis}

From a theoretically point of view it is also highly interesting how such a double adaptive strategy effects the solution. Here we proceed in the following steps:
 
 \begin{enumerate}
 \item Our first analytical inquiry will be the comparison of models~\eqref{L2TVmultimodel} and~\eqref{L2TVmultimodel2}, more specifically the study of the conditions in which it is possible to transform one problem into the other, i.e., whether there is a transformation $\mathcal{Q}$ such that $\mathcal{Q}(\lambda)=\alpha$ (respectively $\mathcal{Q}(\alpha)=\lambda$) such that $u_\lambda = u_\alpha$. 
 
 \item If such a transformation exists, then the follow-up question concerning continuity with respect to certain function spaces is tackled. This has already been studied in $1d$ in~{[Hint]}, 
but the higher dimensional case is still open. A first approach will be to study the Cartesian product case, i.e., the case when $\lambda(x)=\lambda_1(x_1)\times \dots \times \lambda_d(x_d)$.

Complementarily we will consider the dependency of the solutions of~\eqref{L2TVmultimodel} and~\eqref{L2TVmultimodel2} on the parameters  $\alpha$ and $\lambda$. We will study in which function spaces the solutions depend continuously on the regularisation parameters in order to create an "analytical toolbox" of result that will help us in the pursuit of "quantitative results". 
 
 \item Armed with these results we will study the monotonicity and convergence properties of the developed double adaptive methods, i.e., whether the localised resolution increase together with a parameter adjustment, results in optimal (or quasi-optimal) rate of approximation to the solution. 
 \end{enumerate} 
 
\subparagraph{WP 4: Validation and application} 
 
 \textcolor{blue}{How does an h-adaptive and locally adaptive TV method compare with higher order regularizers, as TGV? A digital image is always piecewise constant!}
 
The derived adaptive methods will be validated by carefully planed experiments on image denoising application. In particular we consider the following settings:

\begin{enumerate}[(i)]
\item \textbf{Solely denoising:} In these experiments the considered images are solely corrupted by noise, where they may contain Gaussian noise, impulsive noise or a combination of these two noise types.

\item \textbf{Deblurring $\&$ denoising}: Images might not only be corrupted by noise but also by blur. Here we consider such images, where the noise may vary as in (i).

\item \textbf{MRI:} In magnetic resonance imaging one wishes to reconstruct an image which is only given by partial Fourier-data and additionally distorted by some additive Gaussian noise. We will not only test our methods on artificially constructed Fourier-data, but also plan to reconstruct images from real field-data provided by the group of Prof. Martin Uecker (University of G\"ottingen, University Medical Center G\"ottingen).

\item \textbf{Wavelet inpainting:} This is the problem of filling in missing data in the wavelet domain, whereby the data may got lost by conventional communication techniques and noise is added to the data. 

\end{enumerate}
 In order to compare the obtained solutions qualitatively, we need to understand which restoration quality measures are suitable for a non-uniform discretization. Can the standard measures, like PSNR, MSE, and SSIM, be directly used for a non-uniform discretization, representing the visual quality? Considering again the example from Figure \ref{fig:1Dsignal}, we observe that this seems not obviously the case. While the transformation onto the locally refined grid for the reconstruction in Figure \ref{fig:1Dsignal} is done by keeping the value as in the parent element, for the reconstruction in Figure \ref{fig:PSNR} a linear interpolation is performed. In both situations the reconstructions on the non-uniform grid are visually more appealing than the solution on the uniform grid. However, the PSNR-values indicate something different. In particular, for the reconstruction on the non-uniform grid in Figure \ref{fig:PSNR} PSNR = 26.54 and for the one in Figure \ref{fig:1Dsignal} PSNR = 34.74, while for the solution on the uniform grid PSNR = 28.88. Note that a larger PSNR indicates a better restoration. While the PSNR-value for the reconstruction of Figure~\ref{fig:1Dsignal} seem reasonable, as it quantifies our visual observation, the PSNR-value of the restoration in Figure~\ref{fig:PSNR} seems odd. The reason for this counterintuitive behaviour of the PSNR might be that, since at the discontinuity the mesh is locally refined, in the case of linear interpolation there the minimization process aims at a solution which is around 0.5, i.e., rather far away from the correct value. Although the size of the element with value around 0.5 is small, the size of the element is weighted linear and not quadratic as the value at this point. Hence the value is more weighted/influential than the size of the element. In this sense we investigate how to utilize standard quality measures for non-uniform grids, so that a fair quantitative comparison of the restoration quality can be done. \textcolor{blue}{(Is it clear what is meant here? - There might be a visualisation problem in MatLab. It depicts piecewise linear functions rather than piecewise constant.)}
 
% \textcolor{blue}{How to measure the restoration quality? Are usual measures as PSNR, MSE, SSIM suitable for a non-uniform discretization? To ME it seems that PSNR is not suitable. Example will be presented!}
 
\begin{figure}[h!]
\begin{center}
{\includegraphics[width=5cm]{Comparison_uniform_nonuniform_mesh_average}}
\caption{Reconstruction of the noisy signal in Figure \ref{fig:noisy_signal} with a locally adjusted parameter on uniform and non-uniform grid as in Figure \ref{fig:1Dsignal}. PSNR 28.88 (uniform) and PSNR 26.54 (non-uniform)}\label{fig:PSNR}
\end{center}
\end{figure}
 

 
 \subparagraph{WP 5: Documentation and publication}
 
 The project results are presented in reports, at national and international conferences and published in journals. These publications will be written during the entire project period, whenever significant new results are achieved. The software created in the project is documented with modern documentation tools in such a way that reusability is guaranteed. Reusability is not only guaranteed within the group, but as the implementation is done in the software environment DUNE appropriate subpackages of the resulting software are made available to everybody via the internet.






\paragraph{Work plan}
Based on the work packages described above the intended workload is distributed during the project period as follows:\\ 
\begin{center}
%\begin{table}[!htbp]\centering
{%\small%\scriptsize%
\begin{tabular}{p{7cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}}
\toprule
 \multicolumn{1}{c|}{work program} &  \centering year 1 & \centering year 2 & \centering{year 3} \tabularnewline
\midrule
\textbf{WP 1 Indicator} & & & \\%\MyColorBox{1cm}{1pt}{1.8cm}\nolinebreak & & \MyColorBox{1cm}{1pt}{.5cm}\nolinebreak \\% \colorbox{black}{\parbox{3cm}{}} &  \\
\-\hspace{2pt} WP 1.1: Heuristic indicators & \MyColorBox{1.9cm}{3pt}{0.cm}\nolinebreak & & \\ %\multicolumn{3}{l}{\hspace{0.5cm}\textbullet \textbullet \textbullet \textbullet \textbullet \textbullet  \textbullet \textbullet \textbullet } \\
\-\hspace{2pt} WP 1.2: Theoretic indicators & & \MyColorBox{2.4cm}{3pt}{1.7cm}\nolinebreak & \\%\multicolumn{3}{l}{\hspace{3.9cm} \textbullet\textbullet\textbullet\textbullet\textbullet\textbullet\textbullet \textbullet\textbullet\textbullet\textbullet\textbullet\textbullet \textbullet}   \\
\hline
\textbf{WP 2 Methods} & & &  \\
\-\hspace{2pt} WP 2.1  Adaptive FDM& \MyColorBox{0.5cm}{3pt}{0cm} &  & \\% \multicolumn{3}{l}{\textbullet\textbullet\textbullet\textbullet}   \\
\-\hspace{2pt} WP 2.2 Fully adaptive method by FD& \MyColorBox{1.2cm}{3pt}{1.cm}\nolinebreak &  &\MyColorBox{0.5cm}{3pt}{1.2cm}\nolinebreak \\% \multicolumn{3}{l}{\hspace{1.3cm} \textbullet\textbullet\textbullet\textbullet \textbullet\textbullet}   \\
\-\hspace{2pt} WP 2.3 FE discretization & &  \MyColorBox{2cm}{3pt}{.2cm}\nolinebreak &\MyColorBox{0.5cm}{3pt}{0.4cm}\nolinebreak \\%\multicolumn{3}{l}{\hspace{2.3cm} \textbullet\textbullet\textbullet \textbullet \textbullet\textbullet \textbullet\textbullet\textbullet  \textbullet\textbullet\textbullet}   \\
\hline
\textbf{WP 3 Analysis} %& & & \\ \-\hspace{2pt} WP 6 
& \MyColorBox[gray]{5.cm}{3pt}{1.5cm}\nolinebreak && \\% \multicolumn{3}{l}{\hspace{0.9cm} \textbullet\textbullet\textbullet\textbullet\textbullet\textbullet\textbullet \textbullet\textbullet\textbullet\textbullet\textbullet\textbullet\textbullet\textbullet\textbullet \textbullet\textbullet\textbullet \textbullet\textbullet\textbullet \textbullet\textbullet\textbullet \textbullet\textbullet\textbullet \textbullet\textbullet\textbullet}   \\
%(C)  Application and validation & \multicolumn{3}{l}{\hspace{2.4cm} \textbullet\textbullet\textbullet\textbullet\textbullet\textbullet\textbullet \textbullet\textbullet}   \\
\hline
\textbf{WP 4 Validation and application} %& & & \\ \-\hspace{2pt} WP 7 
&  \MyColorBox{1.cm}{3pt}{1.5cm}\nolinebreak &\MyColorBox{1.cm}{3pt}{1.5cm}\nolinebreak & \MyColorBox{.8cm}{3pt}{1cm}\nolinebreak  \\% \multicolumn{3}{l}{\hspace{1.9cm} \textbullet\textbullet\textbullet \textbullet\textbullet\textbullet \hspace{0.8cm}  \textbullet\textbullet\textbullet \hspace{0.4cm} \textbullet\textbullet\textbullet  \textbullet\textbullet  \textbullet\textbullet\textbullet}\\
\hline
\textbf{WP 5 Documentation} %& & & \\ \-\hspace{2pt} WP 8 
&  \MyColorBox{1.cm}{3pt}{1.7cm}\nolinebreak &&  \MyColorBox{1.9cm}{3pt}{0.cm}\nolinebreak \\% \multicolumn{3}{l}{\hspace{1.9cm} \textbullet\textbullet\textbullet \textbullet\textbullet\textbullet \hspace{1.4cm}  \textbullet\textbullet\textbullet  \textbullet\textbullet\textbullet  \textbullet\textbullet  \textbullet\textbullet\textbullet}\\
\bottomrule
\end{tabular}
%\caption{Intended workload distribution over project duration.\label{T:workload}}}\vspace{-0.5cm}
%\end{table}
}
\end{center}
 


\subsection{Impact and Continuation of the Project}


Although our proposed adaptive method is tailored to Gaussian and impulsive noise removal as well as to total variation minimization, is also serves as a groundwork for other type of noises, as Poisson and multiplicative noise, as well as other type of regularization.

\paragraph{Supplements.}\todo{Here could go: nonconvex regularizer, different data terms, ...}
%We are going to consider additional topics for enriching the understanding of adaptive mechanisms by aspects that are different to the setting above. 


%In order to understand better the behavior of the $L^1$-$L^2$-TV model we intend to investigate its edge-preserving and scale-dependent properties for noise-free as well as for noisy data.
%
%We also want to extend the adaptive mechanisms to further applications, as the reconstruction from partial Fourier-data, which is a problem in magnetic resonance imaging, and wavelet inpainting, which is about restoring missing wavelet coefficients due to lossy compression or error-prone data transmission (see \cite{ChaSheZho, ZhaCha}). Note, that in these applications the observed data is not an image but frequencies and wavelet coefficients, respectively. Yet it is not clear at all how an adaptive meshing should be reasonable performed in such problems.
%
%Recently the $L^1$-$L^2$-TV model has been adapted to the $L^1$-$L^2$-TGV model in \cite{LiuShiYu Wan2015} by replacing the total variation term by the total generalized variation. Of course also other types of regularizers might be considered in this context, as the Mumford-Shah regularizer~\cite{GuMoNo}, higher-order regularizer, the nonlocal total variation and the $B_{1,1}^1$ Besov norm~\cite{Oswald:94, Triebel:78}. Note, that computing a minimizer using one of the aforementioned regularizers instead of the the total variation may improve the quality of the solution by the cost of increased computational complexity. Therefore it is indeed interesting to introduce %and in order to improve the restoration quality we want to introduce an 
%adaptive finite element methods for functionals consisting of a combined $L^1$/$L^2$ data fidelity term and one of the before mentioned regularizers.

\todo{Application medical imaging as a continuation project}

\subsection{Other information }
\subsubsection{Responsibilities of the investigators}
\todo{Do not know how to do that reasonably!}

\paragraph{Dr. Andreas Langer}  is an expert in the field of mathematical image reconstruction, optimization and inverse problems. Hence he knows how to tackle the considered applications in an optimization framework and has experience in deriving and analysing locally adaptive parameter selection methods. He will be the primary supervisor of the PhD candidate, who will be situated at the University of Stuttgart. Hence he will be responsible for all parts of the project but primary for WP 1.1, WP 2.1, WP 2.2 and WP 4. %Due to his expertise, he is responsible for WP 1.1.
%\begin{enumerate}[(1)]
%\item the development and analysis of the adaptive discretization techniques;
%\item the development and analysis of the adaptive regularization methods;
%\item the supervision of the postdoc researcher and doctoral student for the derivation of the adaptive methods, their numerical implementation, and their validation  
%\end{enumerate} 
%development and analysis of the adaptive regularisation methods, their implementation and evaluation. % of the image reconstruction problems, finding corresponding convergent methods to obtain a solution, and the numerical implementation and evaluation. 



\paragraph{Dr. Fernando Gaspoz}  has a strong background in functional analysis and is an expert for a posteriori estimates for adaptive finite elements. In this project he is co-supervising the PhD candidate and primary responsible for WP 1.2, WP 2.3 and WP 3. 
%\begin{enumerate}[(1)]
%\item  the derivation of the a posteriori estimates for the respective problems and the adaptive discretization approaches;
%\item the supervision of the postdoc researcher and doctoral student for the derivation of the adaptive finite element methods and analytical aspects involved in the project;
%\end{enumerate}

 
%\begin{enumerate}
%\item Consider clean images, as in [Bartels], [Hinterm\"uller, Rincon] for the $L^2$-TV model, now for the $L^1$-$L^2$-TV model and construct an adaptive finite element method. Is expected to work very similar as in previous work
%\item Noisy images, as in [Hinterm\"uller, Rincon] for the $L^2$-TV model, now for the $L^1$-$L^2$-TV model. May work similar but could be different since it should be able to treat a mixture of noise.
%\item Inpainting (+ noise): An adaptive FEM has not yet be considered for this application. 
%\item wavelet-inpainting (+ noise)
%\item reconstruction from partial Fourier-data (+ noise)
%\item Using different regularizers to improve restoration. For example consider Mumford-Shah regularizer for inpainting, Euler-Elastica model for inpainting, higher order regularizer for any application, nonlocal TV, TGV, ...
%\item Consider different algorithms (first and second order) and compare. For each algorithm one has to calculate aposteriori estimates separately.
%
%\item Find efficient solvers for the adaptive FE discretization, e.g., subspace correction methods.
%\end{enumerate}


% With this proposal we want to continue the successful
% work on the analysis of adaptive methods for \ppdes. 
%The research program described above has a rich and prosperous potential. 

% \subsection{Data handling}
% \label{sec:data-handling}
% Does not apply.

% \subsection{Other information}
% \label{sec:other-information}
% Does not apply.

% \subsection{Descriptions of proposed investigations involving experiments on humans, human materials or animals}
% \label{sec:proposed-investigations}
% Does not apply.


% \subsection{Information on scientific and financial involvement of international cooperation partners}
% \label{sec:cooperations}
% Does not apply.

%\small
\section{Bibliography}%
\makeatletter
\renewcommand{\@biblabel}[1]{[#1]}
\renewcommand*\bib@heading{%
%\subsection{Own Contributions}%
}
\makeatother
\iffalse
\bibliographystyle{acm}
% mbplain
\bibliography{nmh,para}
\else
%
{\small
\begin{thebibliography}{99}\itemsep=-3.5pt

\bibitem{AdaFou}
{\sc Adams, R. A. and Fournier, J. J. F.}
\newblock{\it Sobolev spaces}.
\newblock volume 140 of Pure and Applied Mathematics (Amsterdam). Elsevier/Academic Press, Amsterdam, second edition, 2003.

\bibitem{AinOde}
{\sc Ainsworth, M., and Oden, J.~T.}
\newblock {\em A Posteriori Error Estimation in Finite Element Analysis}.
\newblock Wiley-Interscience, New York, 2000.

%\bibitem{AlkLan}
%{\sc Alk\"amper, M. and Langer, A.}
%\newblock{Using DUNE-ACFem for non-smooth minimization of bounded variation functions}.
%\newblock{\em submitted to Archive of Numerical Software}, (2016), pp. 13.

\bibitem{All}
{\sc Alliney, S.}
\newblock{A property of the minimum vectors of a regularizing functional defined by means of the absolute norm}.
\newblock{\em IEEE Transactions on Signal Processing 45}, 4 (1997), 913--917.
 
\bibitem{AmbFusPal}
{\sc Ambrosio, L., Fusco, N., and Pallara, D.}
\newblock{\it Functions of bounded variation and free discontinuity problems}.
\newblock Oxford Mathematical Monographs. The Clarendon Press, Oxford University Press, New York, 2000.

\bibitem{BanJos} 
{\sc Bangerth, W., and Joshi, A.}
\newblock {Adaptive finite element methods for the solution of inverse problems in optical tomography}.
\newblock {\em Inverse Problems 24}, 3 (2008), pp 22.

\bibitem{BanRan}
{\sc Bangerth, W., and Rannacher, R.}
\newblock {\em Adaptive Finite Element Methods for Differential Equations}.
\newblock Lectures in Mathematics, ETH Z\"urich. Birkh\"auser, Basel, 2003.

\bibitem{BazBlo} 
{\sc Bazan, C and Blomgren, P.}
\newblock {Adaptive finite element method for image processing}.
\newblock {\em Report - Publication number: CSRCR2006-15}, (2006), pp 6.

\bibitem{Bar2012} 
{\sc Bartels, S.}
\newblock {Total variation minimization with finite elements: convergence and iterative solution}.
\newblock {\em SIAM J. Numer. Anal. 50}, 3 (2012), 1162--1180.

\bibitem{Bar2013} 
{\sc Bartels, S.}
\newblock {Broken Sobolev space iteration for total variation regularized minimization problems}.
\newblock {\em IMA Journal of Numerical Analysis}, (2015), pp. 10.

\bibitem{Bar2015} 
{\sc Bartels, S.}
\newblock {Error control and adaptivity for a variational model problem defined on functions of bounded variation}.
\newblock {\em Mathematics of Computation 84}, 293 (2015), 1217--1240.

\bibitem{BarNocSal} 
{\sc Bartels, S., Nochetto, R. H., and Salgado, A. J. }
\newblock {Discrete total variation flows without reglarization}.
\newblock {\em SIAM J. Numer. Anal. 52}, 1 (2014), 363--385.

\bibitem{BaeMik}
{\sc B\"ansch, e. and Mikula, K.}
\newblock{A coarsening finite element strategy in image selective smoothing}.
\newblock{\em Computing and Visualization in Science 1}, (1997), 53--61.

\bibitem{Bov}
{\sc Bovik, A.}
\newblock{\it Handbook of Image and Video Processing}.
\newblock Academic Press, San Diego, 2000.

\bibitem{BreKunPoc}
{\sc Bredies, K. and Kunisch, K. and Pock, T.}
\newblock{Total generalized variation}.
\newblock{\em SIAM Journal on Imaging Sciences 3}, 3 (2010), 492--526.

\bibitem{Car}
{\sc Carey, G.~F.}
\newblock {\em Computational Grids: Generation, Adaptation and Solution Strategies}.
\newblock Taylor and Francis, London, 1997.

\bibitem{CatLioMorCol}
{\sc Catt\'e, F., Lion, P. L., Morel, J. M., and Coll, T.}
\newblock{Image selective smoothing and edge detection by nonlinear diffusion}.
\newblock{\em SIAM Journal Numer. Anal. 129}, (1992), 182--193.

\bibitem{Cha}
{\sc Chambolle, A.} 
\newblock{An algorithm for total variation minimization and applications}. 
\newblock{\em J. Math. Imaging Vision 20}, 1-2 (2004), 89--97.

\bibitem{ChaDar}
{\sc Chambolle, A. and Darbon, J.} 
\newblock{On total variation minimization and surface evolution using parametric maximum flows}. \newblock{\em International journal of computer vision 84}, 3 (2009), 288--307.

\bibitem{ChaLio}
{\sc Chambolle, A. and Lions,P.-L.}
\newblock{ Image recovery via total variation minimization and related problems}. 
\newblock{\em Numer. Math. 76}, 2 (1997), 167--188.

\bibitem{ChaPoc}
{\sc Chambolle, A. and Pock, T.} 
\newblock{A first-order primal-dual algorithm for convex problems with applications to imaging}. 
\newblock{\em J. Math. Imaging Vision 40}, 1 (2011), 120--145.

\bibitem{ChaHoNik}
{\sc Chan, R. H. and Ho, Ch.-W. and Nikolova, M.}
\newblock{Salt-and-pepper noise removal by median-type noise detectors and detail-preserving regularization},
\newblock{\em IEEE Transactions on Image Processing 14}, 10 (2005), 1479--1485

\bibitem{ChaGolMul}
{\sc Chan, T. F., Golub, G. H., and Mulet P.}
\newblock{ A nonlinear primal-dual method for total variation-based image restoration}. 
\newblock{\em SIAM J. Sci. Comput. 20}, 6 (1999), 1964--1977.

\bibitem{ChaKanShe}
{\sc Chan, T. F., Kang, S. H., and Shen J.}
\newblock{ Euler's Elastica and curvature-based inpainting}. 
\newblock{\em SIAM J. Appl. Math. 63}, 2 (2002), 564--592.

\bibitem{ChaSheZho}
{\sc Chan, T. F., Shen, J., and Zhou, H.-M.}
\newblock{Total variation wavelet inpainting}. 
\newblock{\em J. Math. Imaging Vision 1}, 25 (2006), 107--125.

\bibitem{Ciarlet:02}
{\sc Ciarlet, P.~G.}
\newblock {\em The finite element method for elliptic problems}, vol.~40 of  {\em Classics in Applied Mathematics}.
\newblock Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2002.

\bibitem{ComWaj}
{\sc Combettes, P. L. and Wajs, V. R.} 
\newblock{Signal recovery by proximal forward-backward splitting}.
\newblock{\em Multiscale Model. Simul. 4}, 4 (2005), 1168--1200.

\bibitem{DarSig05}
{\sc Darbon, J. and Sigelle, M.}
\newblock{ A fast and exact algorithm for total variation minimization}.
\newblock{\em In Pattern recognition and image analysis}, Springer (2005), 351--359.

\bibitem{DarSig06}
{\sc Darbon, J. and Sigelle, M.}
\newblock{ Image restoration with discrete constrained total variation. I. Fast and exact optimization.}
\newblock{ \em  J. Math. Imaging Vision 26}, 3 (2006), 261--276.

\bibitem{DauTesVes}
{\sc Daubechies, I., Teschke, G., and Vese, L.} 
\newblock{Iteratively solving linear inverse problems under general convex constraints.} 
\newblock{\em Inverse Probl. Imaging 1}, 1 (2007), 29--46.

\bibitem{DesJaoSel}
{\sc Destuynder, Ph., Jaoua, M., and Sellami, H.}
\newblock{An error estimate in image processing.} 
\newblock{ \em ARIMA Journal 15}, (2012), 61--81.

\bibitem{DobVog}
{\sc Dobson, D. C. and Vogel, C. R.}
\newblock{ Convergence of an iterative method for total variation denoising.} 
\newblock{ \em SIAM J. Numer. Anal. 34} 5 (1997), 1779--1791.

\bibitem{EngHanNeu}
{\sc Engl, H. W., Hanke, M., and Neubauer, A.}
\newblock{\it Regularization of inverse problems}.
\newblock Mathematics and its Applications. Kluwer Academic Publishers Group, Dordrecht, 1996.

\bibitem{GabMer}
{\sc Gabay, D. and Mercier, B.}
\newblock{ A dual algorithm for the solution of nonlinear variational problems via finite-element approximations.} 
\newblock{ \em Comput. Optim. Appl. 2} (1976), 17--40.

\bibitem{Giu}
{\sc Giusti, E.}
\newblock{\it Minimal surfaces and functions of bounded variation}
\newblock volume 80 of Monographs in Mathematics. Birkh\"auser Verlag, Basel, 1984

\bibitem{Glo}
{\sc Glowinski, R.}
\newblock{\it Numerical Methods for Nonlinear Variational Problems}.
\newblock Springer-Verlag, New York, 1984.

\bibitem{GolOsh}
{\sc Goldstein, T. and Osher, S.} 
\newblock{The split Bregman method for $L^1$-regularized problems.} 
\newblock{\em SIAM J. Imaging Sci. 2} 2 (2009), 323--343.

\bibitem{GonSheToh2014} 
{\sc Gong, Z., Shen, Z., and Toh, K.-C.}
\newblock {Image restoration with mixed or unknown noises}.
\newblock {\em Multiscale Model. Simul. 12}, 2 (2014), 458--487.

\bibitem{GuMoNo}
{\sc Do{\v{g}}an, G. , Morin, P. and Nochetto, R. H.} 
\newblock{A variational shape optimization approach for image
              segmentation with a {M}umford-{S}hah functional.} 
\newblock{\em SIAM J. Sci. Comput. 30} 6 (2008), 3028--3049.

%\bibitem{HinLan2013} 
%{\sc Hinterm\"uller, M., and Langer, A.}
%\newblock {Subspace correction methods for a class of nonsmooth and nonadditive convex variational problems with mixed $L^1/L^2$ data-fidelity in image processing}.
%\newblock {\em SIAM J. Imaging Sciences 6}, 4 (2013), 2134--2173.

\bibitem{HinRin2014} 
{\sc Hinterm\"uller, M., and Rincon-Camacho, M.}
\newblock {An adaptive finite element method in $L^2$-TV-based image denoising}.
\newblock {\em Inverse Problems and Imaging 8}, 3 (2014), 685--711.

\bibitem{HinSta}
{\sc Hinterm\"uller, M. and Stadler, G.}
\newblock{ An infeasible primal-dual algorithm for total bounded variation-based inf-convolution-type image restoration.}
\newblock{SIAM J. Sci. Comput. 28}, 1 (2006), 1--23.

\bibitem{KinOshJon}
{\sc Kindermann, S., Osher, S., and Jones, P. W. }
\newblock{Deblurring and denoising of images by nonlocal functionals}.
\newblock {\em Multiscale Model. Simul. 4}, 4 (2005), 1091--1115

\bibitem{KoRoSi:14}
 {\sc Kohls, K.,  R\"osch, A. and  Siebert, K.\,G.}
\newblock A Posteriori error analysis of optimal control problems with control constraints.
\newblock {\em SIAM J. Control Optim.}, 52(3):1832--1861, 2014.

\bibitem{Lam}
{\sc Lamichhane, B.}
\newblock {Finite element techniques for removing the mixture of Gaussian and impulsive noise}.
\newblock {\em IEEE Transactions on Signal Processing 57}, 7 (2009), 2538--2547.

\bibitem{Lan2015} 
{\sc Langer A.}
\newblock {Automated parameter selection for total variation minimization in image restoration}.
\newblock { {\em J. Math. Imaging Vis.}}, (2016), pp. 30.

%\bibitem{Lan2016} 
%{\sc Langer A.}
%\newblock {Automated parameter selection in the $L^1$-$L^2$-TV model for removing Gaussian plus impulse noise}.
%\newblock { submitted to {\em Inverse Problems}}, (2016), pp. 29.

\bibitem{Li} 
{\sc Li, J.}
\newblock{Finite element analysis for a nonlinear diffusion model in image processing}.
\newblock {\em Applied Mathematics Letter 15}, (2002), 197--202.

\bibitem{LiuShiYu Wan2015} 
{\sc Liu, R. W., Shi, L., Yu, S. C. H., and Wang, D.}
\newblock {Box-constrained second-order total generalized variation minimization with a combined $L^{1,2}$ data-fidelity term for image reconstruction}.
\newblock {\em Journal of Electronic Imaging 24}, 3 (2015), 033026--033026.

\bibitem{Lov}
{\sc Love, A. E. H.}
\newblock{\it A Treatise on the Mathematial Theory of Elasticity},
\newblock Dover, 4th ed., 1927.

\bibitem{Moe}
{\sc Moeslund, T. B.}
\newblock{\it Introduction to video and image processing: Building real systems and applications},
\newblock Springer Science \& Business Media, 2012

\bibitem{Nes}
{\sc Nesterov, Y.}
\newblock{Smooth minimization of non-smooth functions.}
\newblock{\em  Math. Program. 103} 1, Ser. A (2005), 127--152.

\bibitem{Nik2002}
{\sc Nikolova, M.}
\newblock{Minimizers of cost-functions involving nonsmooth data-fidelity
              terms. {A}pplication to the processing of outliers}.
 \newblock{\em SIAM Journal on Numerical Analysis 40}, 3 (2002), 965--994.

\bibitem{Nik2004}
{\sc Nikolova, M.}
\newblock{A variational approach to remove outliers and impulse noise}.
 \newblock{\em Journal of Mathematical Imaging and Vision 20}, 1-2 (2004), 99--120.
 
\bibitem{NoSiVe:09}
{\sc Nochetto, R.~H., Siebert, K.~G., and Veeser, A.}
\newblock Theory of adaptive finite element methods: an introduction.
\newblock In {\em Multiscale, nonlinear and adaptive approximation}. Springer, Berlin, 2009, pp.~409--542.


\bibitem{OshBurGolXuYin}
{\sc Osher, S., Burger, M., Goldfarb, D., Xu, J., and Yin, W.}
\newblock{ An iterative regularization method for total variation-based image restoration.} 
\newblock{\em Multiscale Model. Simul. 4}, 2 (2005), 460--489.

\bibitem{Oswald:94}
{\sc Oswald, P.}
\newblock {\em Multilevel finite element approximation: Theory and applications}.
\newblock Teubner Skripten zur Numerik. B. G. Teubner, Stuttgart, 1994.

\bibitem{PapSch}
{\sc Papafitsoros, K. and Sch\"onlieb, C. B.}
\newblock{A combined first and second order variationsl approach for image reconstruction}.
\newblock{\em Journal of mathematical imaging and vision 48}, 2 (2014), 308--338.

\bibitem{PerMal}
{\sc Perona, P. and Malik, J. }
\newblock{Scale-space and edge detection using anisotropic diffusion.}
\newblock{\em IEEE Transactions on Pattern Analysis and Machine Intelligence 12}, 7 (1990), 629--639.


\bibitem{PreRum}
{\sc Preu\ss er, T. and Rumpf, M. }
\newblock{An adaptive finite element method for large scale image processing}.
\newblock{\em Journal of Visual Communication and Image Representation 11}, (2000), 183--195.


\bibitem{Repin} 
{\sc Repin, S. I.}
\newblock{A posteriori error estimation for variational problems with
              uniformly convex functionals}.
\newblock{Math. Comp.  69},230 (2000), 481--500.

\bibitem{ROF} 
{\sc Rudin, L. I., Osher, S., and Fatemi, E.}
\newblock{Nonlinear total variation based noise removal algorithms}.
\newblock{Physica D: Nonlinear Phenomena 60},1 (1992), 259--268.

\bibitem{Verfurth:96}
{\sc Verf\"urth, R.}
\newblock {\em A Review of A Posteriori Error Estimation and Adaptive  Mesh-Refinement Techniques}.
\newblock Adv. Numer. Math. John Wiley, Chichester, UK, 1996.

\bibitem{TiaYua}
{\sc Tian, W. Y. and Yuan, X.}
\newblock{ Convergence analysis of primal-dual based methods for total variation minimization with finite element approximation.}
\newblock{\em preprint}, (2015), pp. 27.

\bibitem{Triebel:78}
{\sc Triebel, H.}
\newblock {\em Interpolation theory, function spaces, differential operators}.
\newblock North-Holland Mathematical Library, 18. North-Holland Publishing Co., Amsterdam-New York, 1978.


\bibitem{WeiBlaAub}
{\sc Weiss, P., Blanc-Feraud, L., and Aubert, G.}
\newblock{ Efficient schemes for total variation minimization under constraints in image processing.}
\newblock{\em SIAM J. Sci. Comput. 31}, 3 (2009), 2047--2080.

\bibitem{Yao}
{\sc Yao, C. H.}
\newblock{Finite element approximation for {TV} regularization.}
\newblock{\em International Journal of Numerical Analysis and Modeling 5}, 3 (2008), 516--526.

\bibitem{ZhaCha}
{\sc Zhang, X. and Chan, T. F.}
\newblock{Wavelet inpainting by nonlocal total variation.}
\newblock{\em Inverse Problems and Imaging 4}, 1 (2010), 191--210.

%\bibitem{AinsworthOden:00}
%{\sc Ainsworth, M., and Oden, J.~T.}
%\newblock {\em A Posteriori Error Estimation in Finite Element Analysis}.
%\newblock Wiley-Interscience, New York, 2000.
%
%\bibitem{BabuskaMakridakis:97}
%{\sc Babu{\v{s}}ka, I., and Makridakis, C.}
%\newblock On the stability of the discontinuous {G}alerkin method for the heat
%  equation.
%\newblock {\em SIAM J. Numer. Anal. 34\/} (1997), 389--401.
%
%\bibitem{BabuskaStrouboulis:01}
%{\sc Babu{\v{s}}ka, I., and Strouboulis, T.}
%\newblock {\em The finite element method and its reliability}.
%\newblock Numerical Mathematics and Scientific Computation. The Clarendon Press
%  Oxford University Press, 2001.
%
%\bibitem{Babuska:71}
%{\sc Babu\v{s}ka, I.}
%\newblock Error-bounds for finite element method.
%\newblock {\em Numer. Math. 16\/} (1971), 322--333.
%
%\bibitem{BaiocchiBrezzi:83}
%{\sc Baiocchi, C., and Brezzi, F.}
%\newblock Optimal error estimates for linear parabolic problems under minimal
%  regularity assumptions.
%\newblock {\em Calcolo 20}, 2 (1983), 143--176.
%
%\bibitem{BangerthRannacher:03}
%{\sc Bangerth, W., and Rannacher, R.}
%\newblock {\em Adaptive Finite Element Methods for Differential Equations}.
%\newblock Lectures in Mathematics, ETH Z\"urich. Birkh\"auser, Basel, 2003.
%
%\bibitem{BankSantos:93}
%{\sc Bank, R.~E., and Santos, R.~F.}
%\newblock Analysis of some moving space-time finite element methods.
%\newblock {\em SIAM J. Numer. Anal. 30}, 1 (1993), 1--18.
%
%\bibitem{BankYserentant:14}
%{\sc Bank, R.~E., and Yserentant, H.}
%\newblock On the {$H^1$}-stability of the {$L_2$}-projection onto finite
%  element spaces.
%\newblock {\em Numerische Mathematik 126}, 2 (2014), 361--381.
%
%\bibitem{BaKaMa:13}
%{\sc B{\"a}nsch, E., Karakatsani, F., and Makridakis, C.}
%\newblock The effect of mesh modification in time on the error control of fully
%  discrete approximations for parabolic equations.
%\newblock {\em Appl. Numer. Math. 67\/} (2013), 35--63.
%
%\bibitem{BaMoNo:02}
%{\sc B{\"a}nsch, E., Morin, P., and Nochetto, R.~H.}
%\newblock An adaptive {U}zawa {FEM} for the {S}tokes problem: convergence
%  without the inf-sup condition.
%\newblock {\em SIAM J. Numer. Anal. 40}, 4 (2002), 1207--1229.
%
%\bibitem{dune:06}
%{\sc Bastian, P., Blatt, M., Dedner, A., Engwer, C., Kl\"ofkorn, R.,
%  Kuttanikkad, S., {Ohl\-ber\-ger}, M., and Sander, O.}
%\newblock {The Distributed and Unified Numerics Environment (DUNE)}.
%\newblock In {\em Proc. of the 19th Symposium on Simulation Technique in
%  Hannover, Sep. 12 - 14\/} (2006).
%
%\bibitem{BeckerMao:11b}
%{\sc Becker, R., and Mao, S.}
%\newblock Quasi-optimality of adaptive nonconforming finite element methods for
%  the {S}tokes equations.
%\newblock {\em SIAM J. Numer. Anal. 49}, 3 (2011), 970--991.
%
%\bibitem{BeckerMao:11a}
%{\sc Becker, R., and Mao, S.}
%\newblock Quasi-optimality of an adaptive finite element method for an optimal
%  control problem.
%\newblock {\em Comput. Methods Appl. Math. 11}, 2 (2011), 107--128.
%
%\bibitem{BeRa:96}
%{\sc Becker, R., and Rannacher, R.}
%\newblock Weighted a posteriori error control in {FE} methods.
%\newblock {\em Lecture ENUMATH-95\/} (1996).
%
%\bibitem{BeckerRannacher:01}
%{\sc Becker, R., and Rannacher, R.}
%\newblock An optimal control approach to a posteriori error estimation in
%  finite element methods.
%\newblock {\em Acta Numer. 10\/} (2001), 1--102.
%
%\bibitem{BeDiKr:12}
%{\sc Belenki, L., Diening, L., and Kreuzer, C.}
%\newblock Optimality of an adaptive finite element method for the
%  $p$-{L}aplacian equation.
%\newblock {\em IMA J. of Numer. Anal. 32}, 2 (2012), 484--510.
%
%\bibitem{BernardiHechtVerfurth:09}
%{\sc Bernardi, C., Hecht, F., and Verf{\"u}rth, R.}
%\newblock A finite element discretization of the three-dimensional
%  {N}avier-{S}tokes equations with mixed boundary conditions.
%\newblock {\em M2AN Math. Model. Numer. Anal. 43}, 6 (2009), 1185--1201.
%
%\bibitem{Berrone:06}
%{\sc Berrone, S.}
%\newblock Robust a posteriori error estimates for finite element
%  discretizations of the heat equation with discontinuous coefficients.
%\newblock {\em M2AN Math. Model. Numer. Anal. 40}, 6 (2006), 991--1021.
%
%\bibitem{BiDaDe:04}
%{\sc Binev, P., Dahmen, W., and DeVore, R.}
%\newblock Adaptive finite element methods with convergence rates.
%\newblock {\em Numer. Math 97\/} (2004), 219--268.
%
%\bibitem{BiDaDePe:02}
%{\sc Binev, P., Dahmen, W., DeVore, R., and Petrushev, P.}
%\newblock Approximation classes for adaptive methods.
%\newblock {\em Serdica Math. J. 28}, 4 (2002), 391--416.
%\newblock Dedicated to the memory of Vassil Popov on the occasion of his 60th
%  birthday.
%
%\bibitem{BinevDeVore:04}
%{\sc Binev, P., and DeVore, R.}
%\newblock Fast computation in adaptive tree approximation.
%\newblock {\em Numer. Math. 97}, 2 (2004), 193--217.
%
%\bibitem{BonitoNochetto:10}
%{\sc Bonito, A., and Nochetto, R.~H.}
%\newblock Quasi-optimal convergence rate of an adaptive discontinuous
%  {G}alerkin method.
%\newblock {\em SIAM J. Numer. Anal. 48}, 2 (2010), 734--771.
%
%\bibitem{Braess:13}
%{\sc Braess, D.}
%\newblock {\em Finite Elemente: Theorie, schnelle {L\"o}ser und {A}nwendungen
%  in der {E}lastizit{\"a}tstheorie}, 5. {\"u}berarb. aufl.~ed.
%\newblock Springer Spektrum, Berlin, 2013.
%
%\bibitem{BrCaHo:07}
%{\sc Braess, D., Carstensen, C., and Hoppe, R. H.~W.}
%\newblock Convergence analysis of a conforming adaptive finite element method
%  for an obstacle problem.
%\newblock {\em Numer. Math. 107}, 3 (2007), 455--471.
%
%\bibitem{BrPaSt:02}
%{\sc Bramble, J.~H., Pasciak, J.~E., and Steinbach, O.}
%\newblock On the stability of the ${L^2}$ projection in ${H^1(\Omega)}$.
%\newblock {\em Math. Comput. 71}, 237 (2002), 147--156.
%
%\bibitem{BrennerScott:08}
%{\sc Brenner, S.~C., and Scott, L.~R.}
%\newblock {\em The mathematical theory of finite element methods}, third~ed.,
%  vol.~15 of {\em Texts in Applied Mathematics}.
%\newblock Springer, New York, 2008.
%
%\bibitem{bddko:06}
%{\sc Burri, A., Dedner, A., Diehl, D., Kl\"ofkorn, R., and Ohlberger, M.}
%\newblock A general object oriented framework for discretizing non-linear
%  evolution equations.
%\newblock In {\em Advances in High Performance Computing and Computational
%  Sciences}, Y.~S. et~al., Ed., vol.~93. Springer, 2006, pp.~69--87.
%
%\bibitem{CaGeMe:13}
%{\sc Cangiani, A., Georgoulis, E.~H., and Metcalfe, S.}
%\newblock Adaptive discontinuous Galerkin methods for nonstationary convection/diffusion
%	problems. {\em IMA J. of Numer. Anal.}
%  51 (2013), 2911--2934.
%
%\bibitem{Carstensen:02}
%{\sc Carstensen, C.}
%\newblock Merging the {B}ramble-{P}asciak-{S}teinbach and the
%  {C}rouzeix-{T}hom\'ee criterion for {$H^1$}-stability of the
%  {$L^2$}-projection onto finite element spaces.
%\newblock {\em Math. Comp. 71}, 237 (2002), 157--163.
%
%\bibitem{Carstensen:04}
%{\sc Carstensen, C.}
%\newblock An adaptive mesh-refining algorithm allowing for an {$H^1$} stable
%  {$L^2$} projection onto {C}ourant finite element spaces.
%\newblock {\em Constr. Approx. 20}, 4 (2004), 549--564.
%
%\bibitem{CarstensenHoppe:05}
%{\sc Carstensen, C., and Hoppe, R. H.~W.}
%\newblock Convergence analysis of an adaptive edge finite element method for
%  the {2D} eddy current equations.
%\newblock {\em J. Numer. Math. 13}, 1 (2005), 19--32.
%
%\bibitem{CarstensenHoppe:06}
%{\sc Carstensen, C., and Hoppe, R. H.~W.}
%\newblock Error reduction and convergence for an adaptive mixed finite element
%  method.
%\newblock {\em Math. Comp. 75}, 255 (2006), 1033--1042.
%
%\bibitem{CaKrNoSi:08}
%{\sc Cascon, J.~M., Kreuzer, C., Nochetto, R.~H., and Siebert, K.~G.}
%\newblock Quasi-optimal convergence rate for an adaptive finite element method.
%\newblock {\em SIAM J. Numer. Anal. 46}, 5 (2008), 2524--2550.
%
%\bibitem{CasconNochetto:12}
%{\sc Casc{\'o}n, J.~M., and Nochetto, R.~H.}
%\newblock Quasioptimal cardinality of {AFEM} driven by nonresidual estimators.
%\newblock {\em IMA J. Numer. Anal. 32}, 1 (2012), 1--29.
%
%\bibitem{CaNoSi:07}
%{\sc Casc\'on, J.~M., Nochetto, R.~H., and Siebert, K.~G.}
%\newblock Design and convergence of {AFEM} in {H}({\rm div}).
%\newblock {\em Math. Models Methods Appl. 17\/} (2007), 1849--1881.
%
%\bibitem{Cea:64}
%{\sc Cea, J.}
%\newblock Approximation variationnelle des probl{\`e}mes aux limites.
%\newblock {\em Ann. Inst. Fourier 14}, 2 (1964), 345--444.
%
%\bibitem{ChXuHo:10}
%{\sc Chen, H., Xu, X., and Hoppe, R. H.~W.}
%\newblock Convergence and quasi-optimality of adaptive nonconforming finite
%  element methods for some nonsymmetric and indefinite problems.
%\newblock {\em Numer. Math. 116}, 3 (2010), 383--419.
%
%\bibitem{ChHoXu:09}
%{\sc Chen, L., Holst, M., and Xu, J.}
%\newblock Convergence and optimality of adaptive mixed finite element methods.
%\newblock {\em Math. Comp. 78}, 265 (2009), 35--53.
%
%\bibitem{ChenFeng:04}
%{\sc Chen, Z., and Feng, J.}
%\newblock An adaptive finite element algorithm with reliable and efficient
%  error control for linear parabolic problems.
%\newblock {\em Math. Comp. 73\/} (2004), 1167--1193.
%

%
%\bibitem{CoDaDe:02}
%{\sc Cohen, A., Dahmen, W., and DeVore, R.}
%\newblock Adaptive wavelet methods. {II}. {B}eyond the elliptic case.
%\newblock {\em Found. Comput. Math. 2}, 3 (2002), 203--245.
%
%\bibitem{DautrayLions:92}
%{\sc Dautray, R., and Lions, J.-L.}
%\newblock {\em Mathematical analysis and numerical methods for science and
%  technology. {V}ol. 5}.
%\newblock Springer-Verlag, Berlin, 1992.
%\newblock Evolution problems. I, With the collaboration of Michel Artola,
%  Michel Cessenat and H{\'e}l{\`e}ne Lanchon, Translated from the French by
%  Alan Craig.
%
%\bibitem{DebrabantLang:13}
%{\sc Debrabant, K., and Lang, J.}
%\newblock On global error estimation and control of finite difference solutions
%  for parabolic equations.
%\newblock In {\em VI International Conference on Adaptive Modeling and
%  Simulation {ADMOS} 2013\/} (2013), J.~P.~M. de~Almeida, P.~Diez, C.~Tiago,
%  and N.~Par\'es, Eds., Center for Numerical Methods in Engineering {(CIMNE)},
%  Barcelona, Spain, pp.~187--198.
%
%\bibitem{DeLaMa:09}
%{\sc Demlow, A., Lakkis, O., and Makridakis, C.}
%\newblock A posteriori error estimates in the maximun norm for parabolic
%  problems.
%\newblock {\em SIAM J. Numer. Anal. 47}, 3 (2009), 2157--2176.
%
%\bibitem{DemStev:11}
%{\sc Demlow, A., and Stevenson, R.}
%\newblock Convergence and quasi-optimality of an adaptive finite element method
%  for controlling {$L_2$} errors.
%\newblock {\em Numer. Math. 117}, 2 (2011), 185--218.
%
%\bibitem{DieningKreuzer:08}
%{\sc Diening, L., and Kreuzer, C.}
%\newblock Convergence of an adaptive finite element method for the
%  $p$-{L}aplacian equation.
%\newblock {\em SIAM J. Numer. Anal. 46}, 2 (2008), 614--638.
%
%\bibitem{DiKrSt:13}
%{\sc Diening, L., Kreuzer, C., and Stevenson, R.}
%\newblock Instance optimality of the adaptive maximum strategy.
%\newblock {\em Preprint-Arxiv\/} (2013).
%
%\bibitem{Dorfler:96}
%{\sc D{\"o}rfler, W.}
%\newblock A convergent adaptive algorithm for {P}oisson's equation.
%\newblock {\em SIAM J. Numer. Anal. 33}, 3 (1996), 1106--1124.
%
%\bibitem{DouglasDupont:70}
%{\sc Douglas, Jr., J., and Dupont, T.}
%\newblock Galerkin methods for parabolic equations.
%\newblock {\em SIAM J. Numer. Anal. 7\/} (1970), 575--626.
%
%\bibitem{DuXie:12}
%{\sc Du, S., and Xie, X.}
%\newblock Error reduction, convergence and optimality of an adaptive mixed
%  finite element method.
%\newblock {\em J. Syst. Sci. Complex. 25}, 1 (2012), 195--208.
%
%\bibitem{Dupont:82}
%{\sc Dupont, T.}
%\newblock Mesh modification for evolution equations.
%\newblock {\em Math. Comp. 39}, 159 (1982), 85--107.
%
%\bibitem{ErikssonJohnson:91}
%{\sc Eriksson, K., and Johnson, C.}
%\newblock Adaptive finite element methods for parabolic problems {I}: {A}
%  linear model problem.
%\newblock {\em SIAM J. Numer. Anal. 28\/} (1991), 43--77.
%
%\bibitem{ErikssonJohnson:95a}
%{\sc Eriksson, K., and Johnson, C.}
%\newblock Adaptive finite element methods for parabolic problems. {IV}.
%  {N}onlinear problems.
%\newblock {\em SIAM J. Numer. Anal. 32}, 6 (1995), 1729--1749.
%
%\bibitem{ErikssonJohnson:95b}
%{\sc Eriksson, K., and Johnson, C.}
%\newblock Adaptive finite element methods for parabolic problems. {V}.
%  {L}ong-time integration.
%\newblock {\em SIAM J. Numer. Anal. 32}, 6 (1995), 1750--1763.
%
%\bibitem{FeFuPr:14}
%{\sc Feischl, M., F{\"u}hrer, T., and Praetorius, D.}
%\newblock Adaptive {FEM} with optimal convergence rates for a certain class of
%  nonsymmetric and possibly nonlinear problems.
%\newblock {\em SIAM J. Numer. Anal. 52}, 2 (2014), 601--625.
%
%\bibitem{GarauMorin:11}
%{\sc Garau, E.~M., and Morin, P.}
%\newblock Convergence and quasi-optimality of adaptive {FEM} for {S}teklov
%  eigenvalue problems.
%\newblock {\em IMA J. Numer. Anal. 31}, 3 (2011), 914--946.
%
%\bibitem{GaMoZu:12}
%{\sc Garau, E.~M., Morin, P., and Zuppa, C.}
%\newblock Quasi-optimal convergence rate of an {AFEM} for quasi-linear problems
%  of monotone type.
%\newblock {\em Numer. Math. Theory Methods Appl. 5}, 2 (2012), 131--156.
%
%\bibitem{GaspozMorin:09}
%{\sc Gaspoz, F.~D., and Morin, P.}
%\newblock Convergence rates for adaptive finite elements.
%\newblock {\em IMA J. Numer. Anal. 29}, 4 (2009), 917--936.
%
%\bibitem{GeLaVi:11}
%{\sc Georgoulis, E.~H., Lakkis, O., and Virtanen, J.~M.}
%\newblock A posteriori error control for discontinuous galerkin methods for
%  parabolic problems.
%\newblock {\em SIAM J. Numer. Anal. 49}, 2 (2011), 427--458.
%
%\bibitem{HuangXu:12}
%{\sc Huang, J., and Xu, Y.}
%\newblock Convergence and complexity of arbitrary order adaptive mixed element
%  methods for the {P}oisson equation.
%\newblock {\em Sci. China Math. 55}, 5 (2012), 1083--1098.
%
%\bibitem{KaPaPr:13}
%{\sc Karkulik, M., Pavlicek, D., and Praetorius, D.}
%\newblock On 2{D} newest vertex bisection: optimality of mesh-closure and
%  {$H^1$}-stability of {$L_2$}-projection.
%\newblock {\em Constr. Approx. 38}, 2 (2013), 213--234.
%
%\bibitem{Kohls:13}
%{\sc Kohls, K.}
%\newblock {\em An Adaptive Finite Element Method for Control-Constrained
%  Optimal Control Problems}.
%\newblock PhD thesis, Fachbereich Mathematik, Unversit\"at Stuttgart, 2013.
%
%\bibitem{KoRoSi:12}
%{\sc Kohls, K., R{\"o}sch, A., and Siebert, K.~G.}
%\newblock A posteriori error estimators for control constrained optimal control
%  problems.
%\newblock In {\em Constrained optimization and optimal control for partial
%  differential equations}, vol.~160 of {\em Internat. Ser. Numer. Math.}
%  Birkh\"auser/Springer Basel AG, Basel, 2012, pp.~431--443.
%
%\bibitem{KoRoSi:13a}
%{\sc Kohls, K., R{\"o}sch, A., and Siebert, K.~G.}
%\newblock Convergence of adaptive finite elements for control constrained
%  optimal control problems.
%\newblock Stuttgarter Mathematische Berichte 2013-011, Preprint SPP1253-153, 2013.
%
%\bibitem{LaMa:06}
%{\sc Lakkis, O., and Makridakis, C.}
%\newblock Elliptic reconstruction and a posteriori error estimates for fully
%  discrete linear parabolic problems.
%\newblock {\em Math. Comp. 75}, 256 (2006), 1627--1658.
%
%\bibitem{MakridakisNochetto:03}
%{\sc Makridakis, C., and Nochetto, R.~H.}
%\newblock Elliptic reconstruction and a posteriori error estimates for
%  parabolic problems.
%\newblock {\em SIAM J. Numer. Anal. 41}, 4 (2003), 1585--1594.
%
%\bibitem{MeidnerVexler:07}
%{\sc Meidner, D., and Vexler, B.}
%\newblock Adaptive space-time finite element methods for parabolic optimization
%  problems.
%\newblock {\em SIAM J. Control Optim. 46}, 1 (2007), 116--142.
%
%\bibitem{Moeller:10b}
%{\sc M\"oller, C.~A.}
%\newblock {\em Adaptive Finite Elements in the Discretization of Parabolic
%  Problems}.
%\newblock PhD thesis, Universit\"at Augsburg, 2010.
%
%\bibitem{MommerStevenson:09}
%{\sc Mommer, M.~S., and Stevenson, R.}
%\newblock A goal-oriented adaptive finite element method with convergence
%  rates.
%\newblock {\em SIAM J. Numer. Anal. 47}, 2 (2009), 861--886.
%
%\bibitem{MoNoSi:00}
%{\sc Morin, P., Nochetto, R.~H., and Siebert, K.~G.}
%\newblock Data oscillation and convergence of adaptive {FEM}.
%\newblock {\em SIAM J. Numer. Anal. 38\/} (2000), 466--488.
%
%\bibitem{MoNoSi:02}
%{\sc Morin, P., Nochetto, R.~H., and Siebert, K.~G.}
%\newblock Convergence of adaptive finite element methods.
%\newblock {\em SIAM Review 44\/} (2002), 631--658.
%
%\bibitem{MoNoSi:03}
%{\sc Morin, P., Nochetto, R.~H., and Siebert, K.~G.}
%\newblock Local problems on stars: {A} posteriori error estimators,
%  convergence, and performance.
%\newblock {\em Math. Comp. 72\/} (2003), 1067--1097.
%
%\bibitem{MoSiVe:07d}
%{\sc Morin, P., Siebert, K.~G., and Veeser, A.}
%\newblock Basic convergence results for conforming adaptive finite elements.
%\newblock {\em Proc. Appl. Math. Mech. 7\/} (2007), 1026001--1026002.
%\newblock Special Issue: Sixth International Congress on Industrial Applied
%  Mathematics (ICIAM07) and GAMM Annual Meeting, Z\"urich 2007.
%
%\bibitem{MoSiVe:07}
%{\sc Morin, P., Siebert, K.~G., and Veeser, A.}
%\newblock Convergence of finite elements adapted for weaker norms.
%\newblock In {\em Applied and industrial mathematics in {I}taly {II}}, vol.~75
%  of {\em Ser. Adv. Math. Appl. Sci.} World Sci. Publ., Hackensack, NJ, 2007,
%  pp.~468--479.
%
%\bibitem{MoSiVe:08}
%{\sc Morin, P., Siebert, K.~G., and Veeser, A.}
%\newblock A basic convergence result for conforming adaptive finite elements.
%\newblock {\em Math. Models Methods Appl. Sci. 18}, 5 (2008), 707--737.
%
%\bibitem{NoScVer:97}
%{\sc Nochetto, R.~H., Schmidt, A., and Verdi, C.}
%\newblock Adapting meshes and time-steps for phase change problems.
%\newblock {\em Rend. Mat. Acc. Lincei Ser. IX}, 8 (1997), 273--292.
%
%\bibitem{NoScVer:00}
%{\sc Nochetto, R.~H., Schmidt, A., and Verdi, C.}
%\newblock A posteriori error estimation and adaptivity for degenerate parabolic
%  problems.
%\newblock {\em Math. Comp. 69\/} (2000), 1--24.
%
%
%\bibitem{PeterBoehm:09}
%{\sc Peter, M.~A., and B{\"o}hm, M.}
%\newblock Multiscale modelling of chemical degradation mechanisms in porous
%  media with evolving microstructure.
%\newblock {\em Multiscale Model. Simul. 7}, 4 (2009), 1643--1668.
%
%\bibitem{Rabus:10}
%{\sc Rabus, H.}
%\newblock A natural adaptive nonconforming {FEM} of quasi-optimal complexity.
%\newblock {\em Comput. Methods Appl. Math. 10}, 3 (2010), 315--325.
%
%\bibitem{SchmichVexler:08}
%{\sc Schmich, M., and Vexler, B.}
%\newblock Adaptivity with dynamic meshes for space-time finite element
%  discretization of parabolic equations.
%\newblock {\em SIAM J. Sci. Comput. 30}, 1 (2008), 369--393.
%
%\bibitem{SchmidtSiebert:05}
%{\sc Schmidt, A., and Siebert, K.~G.}
%\newblock {\em Design of adaptive finite element software}, vol.~42 of {\em
%  Lecture Notes in Computational Science and Engineering}.
%\newblock Springer-Verlag, Berlin, 2005.
%\newblock The finite element toolbox ALBERTA, With 1 CD-ROM (Unix/Linux).
%
%\bibitem{SSHKK}
%{\sc Schmidt, A., Siebert, K.~G., Heine, C.-J., K\"oster, D., and Kriessl, O.}
%\newblock \textsf{ALBERTA}: {A}n adaptive hierarchical finite element toolbox.
%\newblock \texttt{http://www.alberta-fem.de/}.
%\newblock Version 1.2 and 2.0.
%
%\bibitem{SchwabStevenson:09}
%{\sc Schwab, C., and Stevenson, R.}
%\newblock Space-time adaptive wavelet methods for parabolic evolution problems.
%\newblock {\em Math. Comp. 78}, 267 (2009), 1293--1318.
%
%\bibitem{Siebert:12}
%{\sc Siebert, K.~G.}
%\newblock Mathematically founded design of adaptive finite element software.
%\newblock In {\em Multiscale and adaptivity: modeling, numerics and
%  applications}, vol.~2040 of {\em Lecture Notes in Math.} Springer,
%  Heidelberg, 2012, pp.~227--309.
%
%\bibitem{Stevenson:07}
%{\sc Stevenson, R.}
%\newblock Optimality of a standard adaptive finite element method.
%\newblock {\em Found. Comput. Math. 7}, 2 (2007), 245--269.
%
%\bibitem{Tantardini:14}
%{\sc Tantardini, F.}
%\newblock {\em Quasi-Optimality in the Backward {E}uler-{G}alerkin Method for
%  Linear Parabolic Problems}.
%\newblock PhD thesis, Dipartimento di Matematica 'F. Enriques', Universit{\'a}
%  degli Studi di Milano, 01 2014.
%
%\bibitem{Thomee:06}
%{\sc Thom{\'e}e, V.}
%\newblock {\em Galerkin finite element methods for parabolic problems},
%  second~ed., vol.~25 of {\em Springer Series in Computational Mathematics}.
%\newblock Springer-Verlag, Berlin, 2006.
%
%\bibitem{Tomarelli:84}
%{\sc Tomarelli, F.}
%\newblock Regularity theorems and optimal error estimates for linear parabolic
%  {C}auchy problems.
%\newblock {\em Numer. Math. 45}, 1 (1984), 23--50.
%
%\bibitem{Troeltzsch:10}
%{\sc Tr{\"o}ltzsch, F.}
%\newblock {\em Optimal control of partial differential equations}, vol.~112 of
%  {\em Graduate Studies in Mathematics}.
%\newblock American Mathematical Society, Providence, RI, 2010.
%\newblock Theory, methods and applications, Translated from the 2005 German
%  original by J{\"u}rgen Sprekels.
%
%\bibitem{Veeser:02}
%{\sc Veeser, A.}
%\newblock Convergent adaptive finite elements for the nonlinear {L}aplacian.
%\newblock {\em Numer. Math. 92}, 4 (2002), 743--770.
%
%\bibitem{Verfurth:96}
%{\sc Verf\"urth, R.}
%\newblock {\em A Review of A Posteriori Error Estimation and Adaptive
%  Mesh-Refinement Techniques}.
%\newblock Adv. Numer. Math. John Wiley, Chichester, UK, 1996.
%
%\bibitem{Verfurth:03}
%{\sc Verf{\"u}rth, R.}
%\newblock A posteriori error estimates for finite element discretizations of
%  the heat equation.
%\newblock {\em Calcolo 40}, 3 (2003), 195--212.
%
%\bibitem{Verfurth:10}
%{\sc Verf{\"u}rth, R.}
%\newblock A posteriori error analysis of space-time finite element
%  discretizations of the time-dependent {S}tokes equations.
%\newblock {\em Calcolo 47}, 3 (2010), 149--167.
%
%\bibitem{Wheeler:73}
%{\sc Wheeler, M.~F.}
%\newblock A priori {$L_{2}$} error estimates for {G}alerkin approximations to
%  parabolic partial differential equations.
%\newblock {\em SIAM J. Numer. Anal. 10\/} (1973), 723--759.
%
%\bibitem{Wloka:82}
%{\sc Wloka, J.}
%\newblock {\em Partielle {D}ifferentialgleichungen}.
%\newblock B. G. Teubner, Stuttgart, 1982.
%\newblock Sobolevr{\"a}ume und Randwertaufgaben. Mathematische Leitf{\"a}den.
%
%\bibitem{ZhChShWi:12}
%{\sc Zhong, L., Chen, L., Shu, S., Wittum, G., and Xu, J.}
%\newblock Convergence and optimality of adaptive edge finite element methods
%  for time-harmonic {M}axwell equations.
%\newblock {\em Math. Comp. 81}, 278 (2012), 623--642.

\end{thebibliography}}
\fi
\normalsize

\section{Requested modules/funds}
\label{sec:moduls}

\subsection{Basic module}

\subsubsection{Funding for staff}

We apply for
\begin{itemize}\itemsep=0pt
\item 1 TV-L13 (100\%) doctoral student
  for 3 years (at the University of Stuttgart).
\item 1 student research assistant (40h/month; at the University of Stuttgart).
\end{itemize}
\textbf{Justification.} 
The proposed project is an ambitious doctoral thesis subject, which requires a
highly qualified candidate. The position holder will work on all topics
of the wide spectrum covering analysis, design and implementation of adaptive methods. %We plan to fill the position with Mar Runft, M.Sc., who is familiar with total variation minimization and optimisation techniques in general, due to his studies.

The assignment of the students research assistant is to support the proposer and 
doctoral researcher in basic implementation tasks and numerical experiments. 
Additionally, he/she is supposed to assist the researchers of the project in effective
external presentation (web presence, preparation of conference presentations, etc.).


\subsubsection{Funding for travel costs and visiting researchers.}

We apply for \EUR{15000} for three years. The funds are required for intensive scientific exchange of the investigators among themselves but also with external researches, as Massimo Fornasier (TU Munich), Robert Kl\"ofkorn (IRIS, Bergen) Pedro Morin (IMAL, Santa Fe, Argentina), Ricardo Nochetto (University of Maryland), Talal Rahman (Bergen University College), Juan Carlos De los Reyes (EPN Quito),  Carola-Bibiane Sch\"onlieb (University of Cambridge), which are experts in the field of adaptive finite element methods, discontinuous Galerkin methods, mathematical imaging, and parameter selection methods. For these exchange visits we calculate with \EUR{1600} per year. Moreover achieved results will be presented to an expert audience on international conferences. Potential international conferences are SIAM Conference on Imaging Science, GAMM Annual Meeting, European Finite Element Fair, Scale Space and Variational Methods in Computer Vision, Applied Inverse Problem Conference, \dots. In particular, we intend the attendance of 2 conferences each year (1 international and 1 national), where we calculate with \EUR{1600} for an international and \EUR{800} for an national conference. Moreover, we plan for 1 long term visit (2 - 3 month) for the PhD candidate at one of the aforementioned universities, for which we plan \EUR{3000}.

%Christian Kreuzer and R\"udiger Verf\"urth
%(Ruhr-Universit\"at Bochum), Francesca Tantardini and Andreas Veeser (University
%of Milan), and Charalambos Makridakis (University of Sussex) as well as the 
%presentation of results on international conferences.
We plan with the following budget for three years:
\begin{itemize}\itemsep=-1pt
\item Research visits, invitations and attendance of conferences \dotfill\hbox{ \EUR{12000} for three years}

  \item Long term visits  \dotfill\hbox{\EUR{3000} for three years}
\item In total  \dotfill\hbox{\EUR{15000} for three years}
\end{itemize}

\section{Project requirements}
\label{sec:requirements}

\subsection{Employment status information}
\label{sec:empl-stat-inform}
\begin{description}
\item[Fernando D. Gaspoz:] PostDoc, TU Dortmund.
\item[Andreas Langer:] PostDoc, Chair for Numerical Mathematics for High Performance Computing, Institute for Applied Analysis and Numerical  Simulation, Faculty for Mathematics and Physics, University of Stuttgart.
\end{description}
%\begin{description}
%\item[Kunibert G. Siebert:] W3-Professor, Chair for Numerical Mathematics for
%  High Performance Computing, Institute for Applied Analysis and Numerical
%  Simulation, Faculty for Mathematics and Physics, University of Stuttgart.
%\end{description}

\subsection{First-time proposal data}
Gaspoz, Fernando\\
Langer, Andreas

\subsection{Composition of the project group}
\label{sec:comp-proj-group}


The research team is supported by
\begin{itemize}
%\item Dr. Fernando D. Gaspoz, PostDoc at the Chair of Numerical Mathematics for
%  HPC, who is a consultant for approximation classes and optimality of
%  adaptive methods;
\item Dr. Claus-Justus Heine, Akademischer Rat at the University of Stuttgart,
  and Martin Alk\"amper MSc, doctoral student financed by the DFG cluster of excellence
  ``Simulation Technology'', who work intensively on an
  adaptive simulation environment for parabolic and elliptic problems in \dunefem.
\end{itemize}

\subsection{Cooperation with other researchers}
\label{sec:coop-with-other}


\subsubsection{Researchers with whom you have agreed to cooperate on this project}
\label{sec:rese-with-whom}

%We plan to continue our collaboration with group of Andreas Veeser, Francesca
%Tantardini (both University of Milan, Italy), and Christian Kreuzer
%(Ruhr-Universit\"at Bochum). Their findings 
%on the performance of the full space-time discretization serves as a
%benchmark for our time-stepping approach. We also plan to continue our current
%activities with Christian Kreuzer and R\"udiger Verf\"urth (Ruhr-Universit\"at
%Bochum) on the unsteady Stokes system and with Charalambos Makridakis on
%adaptive methods for \ppdes.


\subsubsection{Researchers with whom you have collaborated scientifically within the past three years}
\label{sec:rese-with-whom-1}
Martin Alk\"amper (Stuttgart), Hendrik Dirks (M\"unster), Claus-Justus Heine (Stuttgart), Michael Hinterm\"uller (Berlin), Pedro Morin (Santa Fe, Argentina), Carlos Rautenberg (Berlin), Juan Carlos De los Reyes (Quito), Kunibert G. Siebert (Stuttgart), Andreas Veeser (Milan, Italy), Tao Wu (Berlin)

%Manfred Bischoff (Stuttgart), Klaus Deckelnick (Magdeburg), Rainer Helmig
%(Stuttgart), Mi\-cha\-el Hinze (Hamburg), Pedro Morin (Santa Fe,
%Argentina), Claus-Dieter Munz (Stuttgart), Ricardo H. Nochetto
%(Maryland, USA), Charalambos Makridakis (Brighton, U.K.), Malte
%A. Peter (Augsburg), Christian Rohde (Stuttgart), Arnd R\"osch
%(Duisburg-Essen), Alfred Schmidt (Bremen),
%Andreas Veeser (Milan, Italy), Winnifried Wollner (Hamburg)


\section{Additional information}
\label{sec:addit-inform}

Does not apply.








\end{document}

DFG-Vertrauensdozent der Uni Stuttgart:
Prof. Hans-Joachim Werner
Institut fuer Theoretische Chemie
Tel.: 0711/685 64401
E-Mail: werner@theochem.uni-stuttgart.de
